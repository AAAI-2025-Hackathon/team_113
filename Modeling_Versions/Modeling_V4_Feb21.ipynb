{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaad991-895a-4089-985b-4a5326161d32",
   "metadata": {},
   "source": [
    "# MODELING V3 FEB21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8f11c-83b3-45d3-80d1-a4e2cf8a491f",
   "metadata": {},
   "source": [
    "### LOADING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13a7736-a299-4871-9e8b-6c5b974d1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3568e-e2bb-4c02-a6a8-e0f6a05d77e7",
   "metadata": {},
   "source": [
    "### DEFINING NECESSARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f85494-1666-4466-9653-59e701291302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling_batch(matrices, new_shape):\n",
    "    original_batch, original_height, original_width = matrices.shape  \n",
    "    target_height, target_width = new_shape\n",
    "    pool_size = original_width // target_width\n",
    "    reshaped = matrices.reshape(original_batch, original_height, target_width, pool_size)\n",
    "    pooled = np.max(reshaped, axis=3)\n",
    "    return pooled\n",
    "\n",
    "def count_and_percentage(array):\n",
    "    flattened = array.flatten()\n",
    "    unique, counts = np.unique(flattened, return_counts=True)\n",
    "    total_elements = flattened.size\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    percentage_dict = {key: (value / total_elements) * 100 for key, value in count_dict.items()}\n",
    "    return count_dict, percentage_dict\n",
    "\n",
    "def resize_tensor_bilinear(tensor, target_size, mode='bilinear'):\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    resized = F.interpolate(tensor, size=target_size, mode=mode, align_corners=False)\n",
    "    return resized.squeeze(0)\n",
    "\n",
    "def resize_tensor_nearest(tensor, target_size, mode='nearest'):\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    resized = F.interpolate(tensor, size=target_size, mode=mode)\n",
    "    return resized.squeeze(0)\n",
    "\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    min_val = tensor.min(dim=-1, keepdim=True).values.min(dim=-2, keepdim=True).values\n",
    "    max_val = tensor.max(dim=-1, keepdim=True).values.max(dim=-2, keepdim=True).values\n",
    "    return ((tensor - min_val) / ((max_val - min_val) + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec3269-f6ef-4045-a742-95545ce77fa0",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c05aa-0609-4329-8fcd-f06758ef3d25",
   "metadata": {},
   "source": [
    "##### LOADING ELEVATION, VEGETATION, SOIL VARIABLE AND SOIL COMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1665e65-c33d-4318-94cd-3ba9644e22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPATIAL MASK\n",
    "spatial_mask = np.load(\"Spatial_Mask.npy\")\n",
    "\n",
    "#ELEVATION DATA\n",
    "elevation_data = np.load('elevation_matrix.npy') \n",
    "\n",
    "#VEGETATION DATA\n",
    "vegetation_data = np.load('NLCD2021_OR.npy') \n",
    "\n",
    "#SOIL VARIABLE DATA\n",
    "directory = \"ERA5_matrices\"  \n",
    "npy_files = sorted([f for f in os.listdir(directory) if f.endswith('.npy')])\n",
    "matrices = [np.load(os.path.join(directory, file)) for file in npy_files]\n",
    "data = np.stack(matrices, axis=0)  # Shape: (num_files, height, width)\n",
    "soil_variable_data = np.transpose(data, (1, 0, 2, 3)) \n",
    "\n",
    "#SOIL COMPOSITION DATA\n",
    "data_folder = \"SOLUS\"\n",
    "npy_files = [f for f in os.listdir(data_folder) if f.endswith('.npy')]\n",
    "num_files = len(npy_files)\n",
    "transformed_data_list = []\n",
    "pca = PCA(n_components=1)\n",
    "for file in npy_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    soil_data = np.load(file_path)\n",
    "    reshaped_data = soil_data.reshape(7, -1).T\n",
    "    principal_component = pca.fit_transform(reshaped_data)  \n",
    "    reduced_data = principal_component.reshape(1306, 464)\n",
    "    transformed_data_list.append(reduced_data)\n",
    "soil_composition_data = np.stack(transformed_data_list, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9d42d2-5a07-432c-b28d-07f6279d0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPATIAL MASK : (120, 120)\n",
      "ELEVATION DATA : (10812, 10812)\n",
      "VEGETATION DATA : (15, 4353, 1547)\n",
      "SOIL VARIABLE DATA : (1096, 28, 5, 5)\n",
      "SOIL COMPOSITION DATA : (18, 1306, 464)\n"
     ]
    }
   ],
   "source": [
    "print(f\"SPATIAL MASK : {spatial_mask.shape}\")\n",
    "print(f\"ELEVATION DATA : {elevation_data.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_data.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_data.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0b21-859b-4b23-b0b2-efc98b80b7f2",
   "metadata": {},
   "source": [
    "##### LOADING OUTPUT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43033c4-81aa-45fa-94c5-1671bdc2eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {0.0: 15118676, 1.0: 303439, 2.0: 360285}\n",
      "Percentages: {0.0: 95.79453061638281, 1.0: 1.9226416768045418, 2.0: 2.282827706812652}\n"
     ]
    }
   ],
   "source": [
    "label = np.load('landslide_labels_stored_sequentially_compressed.npz') \n",
    "output_labels = label['matrix']\n",
    "labels_resized = max_pooling_batch(output_labels, (120, 120))  # Output shape: (1096, 120, 120)\n",
    "counts, percentages = count_and_percentage(labels_resized)\n",
    "print(\"Counts:\", counts)  \n",
    "print(\"Percentages:\", percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1775a8cf-79fd-4a99-a6db-2cea7d2395ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT LABELS : (1096, 120, 120)\n"
     ]
    }
   ],
   "source": [
    "print(f\"OUTPUT LABELS : {labels_resized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc3b64-f9b0-49a1-abe1-ae23070081e6",
   "metadata": {},
   "source": [
    "### CONVERTING NUMPY ARRAYS TO TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a468bd-c653-4066-bef2-a51a29a7252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_data = torch.tensor(elevation_data, dtype=torch.float).unsqueeze(0)  # (1, 10812, 10812)\n",
    "vegetation_data = torch.tensor(vegetation_data, dtype=torch.float)             # (15, 4353, 1547)\n",
    "soil_composition_data = torch.tensor(soil_composition_data, dtype=torch.float) # (18, 1306, 464)\n",
    "soil_variable_data = torch.tensor(soil_variable_data, dtype=torch.float)       # (1096, 28, 5, 5)\n",
    "output_labels = torch.tensor(labels_resized, dtype=torch.long)                 # (1096, 120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7486f2a-7043-4e3e-ae15-fa03759ce9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVATION DATA : torch.Size([1, 10812, 10812])\n",
      "VEGETATION DATA : torch.Size([15, 4353, 1547])\n",
      "SOIL VARIABLE DATA : torch.Size([1096, 28, 5, 5])\n",
      "SOIL COMPOSITION DATA : torch.Size([18, 1306, 464])\n",
      "OUTPUT LABELS : torch.Size([1096, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ELEVATION DATA : {elevation_data.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_data.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_data.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_data.shape}\")\n",
    "print(f\"OUTPUT LABELS : {output_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec0fe9-4202-48be-8bdf-5c8115fde526",
   "metadata": {},
   "source": [
    "### PREPROCESSING THE TENSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644f2db-171d-4349-b379-28bd8bf4a2bf",
   "metadata": {},
   "source": [
    "##### TARGET SPATIAL RESOLUTION (HEIGHT,WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4234b43c-845b-4eee-bd66-e6a78355747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (120, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8e9b7-89fe-485c-acf1-666cc992c41c",
   "metadata": {},
   "source": [
    "##### PRE-RESIZE STATIC DATASETS (DO NOT CHANGE BY THE DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6be0a61f-c38c-4036-a03f-570ab208ddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation output shape: torch.Size([1, 120, 120])\n",
      "Vegetation output shape: torch.Size([15, 120, 120])\n",
      "Soil Variable output shape: torch.Size([1096, 28, 120, 120])\n",
      "Soil Composition output shape: torch.Size([18, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Elevation Data: Input assumed to be (batch, 1, 10812, 10812)\n",
    "#    Expected output: (batch, 1, 120, 120)\n",
    "class ElevationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ElevationNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Use a larger stride to quickly reduce the huge input resolution.\n",
    "            nn.Conv2d(1, 8, kernel_size=7, stride=4, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Map back to one channel.\n",
    "            nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Adaptive pooling will output (120,120) regardless of the spatial size.\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((120, 120))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "\n",
    "# 2. Vegetation Data: Input shape (batch, 15, 4353, 1547)\n",
    "#    Expected output: (batch, 15, 120, 120)\n",
    "class VegetationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VegetationNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(15, 32, kernel_size=7, stride=4, padding=3),\n",
    "            nn.ReLU(),\n",
    "            # Map back to 15 channels.\n",
    "            nn.Conv2d(32, 15, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((120, 120))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "\n",
    "# 3. Soil Variable Data: Input shape (batch, 28, 5, 5) with batch=1096\n",
    "#    Expected output: (batch, 28, 120, 120)\n",
    "#    Because the spatial size is very small, we use upsampling.\n",
    "class SoilVariableNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoilVariableNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # A simple convolutional block that preserves the 28 channels.\n",
    "            nn.Conv2d(28, 28, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Use bilinear upsampling to scale from 5x5 to 120x120.\n",
    "        self.upsample = nn.Upsample(size=(120, 120), mode='bilinear', align_corners=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "# 4. Soil Composition Data: Input shape (batch, 18, 1306, 464)\n",
    "#    Expected output: (batch, 18, 120, 120)\n",
    "class SoilCompositionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoilCompositionNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(18, 32, kernel_size=7, stride=4, padding=3),\n",
    "            nn.ReLU(),\n",
    "            # Map back to 18 channels.\n",
    "            nn.Conv2d(32, 18, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((120, 120))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "\n",
    "# Example usage with dummy data:\n",
    "if __name__ == '__main__':\n",
    "    # # Create dummy inputs with the assumed shapes.\n",
    "    # elevation_data = torch.randn(1, 1, 10812, 10812)       # single sample, 1 channel\n",
    "    # vegetation_data = torch.randn(1, 15, 4353, 1547)        # single sample, 15 channels\n",
    "    # soil_variable_data = torch.randn(1096, 28, 5, 5)         # 1096 samples, 28 channels\n",
    "    # soil_composition_data = torch.randn(1, 18, 1306, 464)    # single sample, 18 channels\n",
    "    \n",
    "    # Initialize models.\n",
    "    elev_net = ElevationNet()\n",
    "    veg_net = VegetationNet()\n",
    "    soil_comp_net = SoilCompositionNet()\n",
    "    soil_var_net = SoilVariableNet()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass.\n",
    "        elevation_resized = elev_net(elevation_data)          # expected shape: (1, 1, 120, 120)\n",
    "        vegetation_resized = veg_net(vegetation_data)           # expected shape: (1, 15, 120, 120)\n",
    "        soil_variable_resized = soil_var_net(soil_variable_data)  # expected shape: (1096, 28, 120, 120)\n",
    "        soil_composition_resized = soil_comp_net(soil_composition_data)  # expected shape: (1, 18, 120, 120)\n",
    "        \n",
    "        # Print output shapes.\n",
    "        print(\"Elevation output shape:\", elevation_resized.shape)\n",
    "        print(\"Vegetation output shape:\", vegetation_resized.shape)\n",
    "        print(\"Soil Variable output shape:\", soil_variable_resized.shape)\n",
    "        print(\"Soil Composition output shape:\", soil_composition_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164e301d-43ac-4736-b6e2-5ebccd365eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elevation_resized = resize_tensor_bilinear(elevation_data, target_size)                # -> (1, 120, 120)\n",
    "# vegetation_resized = resize_tensor_nearest(vegetation_data, target_size)                # -> (15, 120, 120)\n",
    "# soil_composition_resized = resize_tensor_bilinear(soil_composition_data, target_size)  # -> (18, 120, 120)\n",
    "# #soil_variable_resized = F.interpolate(soil_variable_data, size=target_size, mode='bilinear', align_corners=False)  # -> (1096, 28, 120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41682232-2ce2-4c7a-9cd6-898b4d9834a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"ELEVATION DATA : {elevation_resized.shape}\")\n",
    "# print(f\"VEGETATION DATA : {vegetation_resized.shape}\")\n",
    "# print(f\"SOIL COMPOSITION DATA : {soil_composition_resized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b99adb-9fb1-4f7b-ad66-32c66fd7c115",
   "metadata": {},
   "source": [
    "##### NORMALIZE THE CONTINOUS FEATURES (ELEVATION AND SOIL COMPOSITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20bcbf75-a339-4d1d-a590-890dbcfaa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_normalized = (elevation_resized - elevation_resized.min()) / (elevation_resized.max() - elevation_resized.min())\n",
    "soil_composition_normalized = torch.stack([normalize_tensor(t1) for t1 in soil_composition_resized])\n",
    "soil_variable_normalized = torch.stack([normalize_tensor(t2) for t2 in soil_variable_resized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bd907e4-5eb6-46e5-9850-9bdf2f8f1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVATION DATA : torch.Size([1, 120, 120])\n",
      "VEGETATION DATA : torch.Size([15, 120, 120])\n",
      "SOIL COMPOSITION DATA : torch.Size([18, 120, 120])\n",
      "SOIL VARIABLE DATA : torch.Size([1096, 28, 120, 120])\n",
      "OUTPUT LABELS : torch.Size([1096, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ELEVATION DATA : {elevation_normalized.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_resized.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_normalized.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_normalized.shape}\")\n",
    "print(f\"OUTPUT LABELS : {output_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeb81b3f-de3e-42ed-bf65-1a266fe6625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "print(torch.isnan(elevation_normalized).any())\n",
    "print(torch.isnan(vegetation_resized).any())\n",
    "print(torch.isnan(soil_composition_normalized).any())\n",
    "print(torch.isnan(soil_variable_data).any())\n",
    "print(torch.isnan(output_labels).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405b845-1859-458e-8a82-607a3cbbd1f0",
   "metadata": {},
   "source": [
    "### CUSTOM DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d769f491-8eed-465c-bd27-a579d6152368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DayDataset(Dataset):\n",
    "    def __init__(self, day_indices, vegetation, elevation, soil_comp, soil_var, labels, target_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          day_indices: list of day indices (e.g., [0, 1, 2, ...])\n",
    "          vegetation: static vegetation data, already resized, shape (15, target_H, target_W)\n",
    "          elevation: static elevation data, already resized, shape (1, target_H, target_W)\n",
    "          soil_comp: static soil composition data, already resized, shape (18, target_H, target_W)\n",
    "          soil_var: daily soil variable data, shape (1096, 28, 5, 5)\n",
    "          labels: daily output labels, shape (1096, target_H, target_W)\n",
    "          target_size: the target spatial size (target_H, target_W) for all inputs\n",
    "        \"\"\"\n",
    "        self.day_indices = day_indices\n",
    "        self.vegetation = vegetation\n",
    "        self.elevation = elevation\n",
    "        self.soil_comp = soil_comp\n",
    "        self.soil_var = soil_var\n",
    "        self.labels = labels\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.day_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day = self.day_indices[idx]\n",
    "        soil_var_day = self.soil_var[day]\n",
    "        # soil_var_net = SoilVariableNet()\n",
    "\n",
    "        \n",
    "        # soil_var_day_resized = resize_tensor_bilinear(soil_var_day, self.target_size)\n",
    "        # soil_variable_normalized = torch.stack([normalize_tensor(t2) for t2 in soil_var_day_resized])\n",
    "        \n",
    "        \n",
    "        input_tensor = torch.cat([self.vegetation, self.elevation, self.soil_comp, soil_var_day], dim=0)\n",
    "        label = self.labels[day]\n",
    "        #print(f\"INPUT TENSOR WITH MISSING VALUE : {torch.isnan(input_tensor).any()}\")\n",
    "        return input_tensor, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd3f3-a662-4635-ad46-ff63c137a3bf",
   "metadata": {},
   "source": [
    "##### DEFINING TRAIN DAYS, VALIDATION DAYS AND TEST DAYS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c88eb50e-6aa5-4b56-b856-6725135fa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_days = list(range(1096))\n",
    "# train_days = list(range(0, 730))\n",
    "# test_days = list(range(730, 1096))\n",
    "# val_days = train_days[-100:] \n",
    "# train_days = train_days[:-100]\n",
    "all_days = list(range(1096))\n",
    "train_days = list(range(0, 730))\n",
    "test_days = list(range(730, 1096))\n",
    "val_days = random.sample(train_days, 100) # Randomly select 100 days from train_days for validation\n",
    "train_days = [day for day in train_days if day not in val_days] # Remove the selected validation days from train_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fd7a0-2744-4422-8118-51a5b41f5a1b",
   "metadata": {},
   "source": [
    "##### CREATING DATASETS AND DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16d5d11f-9dee-410d-8421-61dc00951d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DayDataset(train_days, vegetation_resized, elevation_normalized,\n",
    "#                            soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "# val_dataset = DayDataset(val_days, vegetation_resized, elevation_normalized,\n",
    "#                          soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "# test_dataset = DayDataset(test_days, vegetation_resized, elevation_normalized,\n",
    "#                           soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "\n",
    "\n",
    "train_dataset = DayDataset(train_days, vegetation_resized, elevation_normalized,\n",
    "                           soil_composition_normalized, soil_variable_normalized, output_labels, target_size)\n",
    "val_dataset = DayDataset(val_days, vegetation_resized, elevation_normalized,\n",
    "                         soil_composition_normalized, soil_variable_normalized, output_labels, target_size)\n",
    "test_dataset = DayDataset(test_days, vegetation_resized, elevation_normalized,\n",
    "                          soil_composition_normalized, soil_variable_normalized, output_labels, target_size)\n",
    "\n",
    "\n",
    "# train_dataset = DayDataset(train_days, vegetation_resized, elevation_resized,\n",
    "#                            soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "# val_dataset = DayDataset(val_days, vegetation_resized, elevation_resized,\n",
    "#                          soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "# test_dataset = DayDataset(test_days, vegetation_resized, elevation_resized,\n",
    "#                           soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 16 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecac178-1a05-4820-9545-92dbf8c59c75",
   "metadata": {},
   "source": [
    "### DEFINING FULLY CONVOLUTIONAL NETWORK (FCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4505fdd5-ce9e-4326-af80-9cd0aad6ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shelly/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (enc1): Sequential(\n",
       "    (0): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottom): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec3): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec2): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec1): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Bottom (bottleneck)\n",
    "        self.bottom = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 256, 256, kernel_size=3, padding=1),  # Concatenated channels from enc3\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 128, 128, kernel_size=3, padding=1),  # Concatenated channels from enc2\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64 + 64, 64, kernel_size=3, padding=1),    # Concatenated channels from enc1\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.enc1(x)   # (64, H, W)\n",
    "        p1 = self.pool1(x1) # (64, H/2, W/2)\n",
    "\n",
    "        x2 = self.enc2(p1)  # (128, H/2, W/2)\n",
    "        p2 = self.pool2(x2) # (128, H/4, W/4)\n",
    "\n",
    "        x3 = self.enc3(p2)  # (256, H/4, W/4)\n",
    "        p3 = self.pool3(x3) # (256, H/8, W/8)\n",
    "\n",
    "        # Bottleneck\n",
    "        x_bottom = self.bottom(p3) # (512, H/8, W/8)\n",
    "\n",
    "        # Decoder\n",
    "        d3 = self.upconv3(x_bottom)        # (256, H/4, W/4)\n",
    "        d3 = torch.cat([d3, x3], dim=1)      # (256 + 256, H/4, W/4)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)              # (128, H/2, W/2)\n",
    "        d2 = torch.cat([d2, x2], dim=1)      # (128 + 128, H/2, W/2)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)              # (64, H, W)\n",
    "        d1 = torch.cat([d1, x1], dim=1)      # (64 + 64, H, W)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        out = self.final_conv(d1)          # (num_classes, H, W)\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "in_channels = 15 + 1 + 18 + 28  # As defined in your FCN\n",
    "num_classes = 3\n",
    "model = UNet(in_channels, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9b224ae-a4b6-48f0-b294-8c551500fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FCN(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes):\n",
    "#         \"\"\"\n",
    "#         A simple encoder-decoder Fully Convolutional Network.\n",
    "#         Args:\n",
    "#           in_channels: number of input channels (here, 62)\n",
    "#           num_classes: number of output classes (here, 3 for labels 0, 1, 2)\n",
    "#         \"\"\"\n",
    "#         super(FCN, self).__init__()\n",
    "#         # Encoder: three conv layers with pooling\n",
    "#         self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1   = nn.BatchNorm2d(64)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)  \n",
    "\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.bn2   = nn.BatchNorm2d(128)\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.bn3   = nn.BatchNorm2d(256)\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         # Decoder: upsampling via transpose convolutions\n",
    "#         self.upconv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.upconv3 = nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))    # (64, H, W)\n",
    "#         x = self.pool1(x)                      # (64, H/2, W/2)\n",
    "\n",
    "#         x = F.relu(self.bn2(self.conv2(x)))    # (128, H/2, W/2)\n",
    "#         x = self.pool2(x)                      # (128, H/4, W/4)\n",
    "\n",
    "#         x = F.relu(self.bn3(self.conv3(x)))    # (256, H/4, W/4)\n",
    "#         x = self.pool3(x)                      # (256, H/8, W/8)\n",
    "\n",
    "#         # Decoder (upsample back to original resolution)\n",
    "#         x = self.upconv1(x)                    # (128, H/4, W/4)\n",
    "#         x = self.upconv2(x)                    # (64, H/2, W/2)\n",
    "#         x = self.upconv3(x)                    # (num_classes, H, W)\n",
    "#         return x\n",
    "\n",
    "# in_channels = 15 + 1 + 18 + 28  \n",
    "# num_classes = 3\n",
    "# model = FCN(in_channels, num_classes)\n",
    "# #model = DeepResMultiScaleFCN(in_channels, num_classes)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c6195-e61a-4fe4-90f7-4d635ff33609",
   "metadata": {},
   "source": [
    "### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e6451-a130-45e2-ad77-ec213514a3ef",
   "metadata": {},
   "source": [
    "##### DEFINING EARLY STOPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08b2233f-304f-4445-a228-af7bfe8aa226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68e818-7b1d-47a8-8593-34fd6f9006e6",
   "metadata": {},
   "source": [
    "##### DEFINING THE LOSS FUNCTION AND OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "423e86cc-eda3-4ab1-bc06-1c91b15b4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = torch.tensor([0.9579453061638281, 0.019226416768045418, 0.02282827706812652])\n",
    "weights = 1.0 / freq\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)  \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "early_stopping = EarlyStopping(patience=15, delta=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89e926-a27b-446e-baff-9c1f34bab32d",
   "metadata": {},
   "source": [
    "##### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5743f745-4d24-4478-a1fd-78e0cfd31e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# num_epochs = 1000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs = inputs.to(device)   # shape: (B, 62, 120, 120)\n",
    "#         labels = labels.to(device)   # shape: (B, 120, 120)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)      # shape: (B, 3, 120, 120)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#     epoch_loss = running_loss / len(train_dataset)\n",
    "#     train_losses.append(epoch_loss)\n",
    "\n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)V2\n",
    "#             val_loss += loss.item() * inputs.size(0)\n",
    "#     epoch_val_loss = val_loss / len(val_dataset)\n",
    "#     val_losses.append(epoch_val_loss)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}  Train Loss: {epoch_loss:.4f}  Val Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "#     early_stopping(val_loss, model)\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break\n",
    "\n",
    "# early_stopping.load_best_model(model)\n",
    "\n",
    "# epochs_run = epoch + 1\n",
    "\n",
    "# end = time.time()\n",
    "# duration = (end-start)/60\n",
    "# print(f\"The training Loop ran for {round(duration,2)} minutes for {num_epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3be989f1-8dc4-4fc7-a9bf-c3b931101278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000  Train Loss: 1.0187  Val Loss: 0.9883\n",
      "Epoch 2/1000  Train Loss: 0.8651  Val Loss: 0.8223\n",
      "Epoch 3/1000  Train Loss: 0.7740  Val Loss: 0.7343\n",
      "Epoch 4/1000  Train Loss: 0.7210  Val Loss: 0.7151\n",
      "Epoch 5/1000  Train Loss: 0.7058  Val Loss: 0.6928\n",
      "Epoch 6/1000  Train Loss: 0.6915  Val Loss: 0.6893\n",
      "Epoch 7/1000  Train Loss: 0.6827  Val Loss: 0.6861\n",
      "Epoch 8/1000  Train Loss: 0.6729  Val Loss: 0.6727\n",
      "Epoch 9/1000  Train Loss: 0.6688  Val Loss: 0.6676\n",
      "Epoch 10/1000  Train Loss: 0.6639  Val Loss: 0.6615\n",
      "Epoch 11/1000  Train Loss: 0.6597  Val Loss: 0.6544\n",
      "Epoch 12/1000  Train Loss: 0.6596  Val Loss: 0.6598\n",
      "Epoch 13/1000  Train Loss: 0.6551  Val Loss: 0.6476\n",
      "Epoch 14/1000  Train Loss: 0.6491  Val Loss: 0.6450\n",
      "Epoch 15/1000  Train Loss: 0.6495  Val Loss: 0.6530\n",
      "Epoch 16/1000  Train Loss: 0.6438  Val Loss: 0.6458\n",
      "Epoch 17/1000  Train Loss: 0.6423  Val Loss: 0.6417\n",
      "Epoch 18/1000  Train Loss: 0.6385  Val Loss: 0.6376\n",
      "Epoch 19/1000  Train Loss: 0.6361  Val Loss: 0.6408\n",
      "Epoch 20/1000  Train Loss: 0.6338  Val Loss: 0.6314\n",
      "Epoch 21/1000  Train Loss: 0.6328  Val Loss: 0.6328\n",
      "Epoch 22/1000  Train Loss: 0.6319  Val Loss: 0.6337\n",
      "Epoch 23/1000  Train Loss: 0.6271  Val Loss: 0.6273\n",
      "Epoch 24/1000  Train Loss: 0.6251  Val Loss: 0.6303\n",
      "Epoch 25/1000  Train Loss: 0.6232  Val Loss: 0.6236\n",
      "Epoch 26/1000  Train Loss: 0.6237  Val Loss: 0.6285\n",
      "Epoch 27/1000  Train Loss: 0.6233  Val Loss: 0.6341\n",
      "Epoch 28/1000  Train Loss: 0.6218  Val Loss: 0.6240\n",
      "Epoch 29/1000  Train Loss: 0.6205  Val Loss: 0.6227\n",
      "Epoch 30/1000  Train Loss: 0.6205  Val Loss: 0.6230\n",
      "Epoch 31/1000  Train Loss: 0.6192  Val Loss: 0.6188\n",
      "Epoch 32/1000  Train Loss: 0.6166  Val Loss: 0.6197\n",
      "Epoch 33/1000  Train Loss: 0.6171  Val Loss: 0.6191\n",
      "Epoch 34/1000  Train Loss: 0.6169  Val Loss: 0.6138\n",
      "Epoch 35/1000  Train Loss: 0.6170  Val Loss: 0.6187\n",
      "Epoch 36/1000  Train Loss: 0.6139  Val Loss: 0.6192\n",
      "Epoch 37/1000  Train Loss: 0.6126  Val Loss: 0.6201\n",
      "Epoch 38/1000  Train Loss: 0.6138  Val Loss: 0.6184\n",
      "Epoch 39/1000  Train Loss: 0.6115  Val Loss: 0.6131\n",
      "Epoch 40/1000  Train Loss: 0.6114  Val Loss: 0.6158\n",
      "Epoch 41/1000  Train Loss: 0.6123  Val Loss: 0.6151\n",
      "Epoch 42/1000  Train Loss: 0.6113  Val Loss: 0.6158\n",
      "Epoch 43/1000  Train Loss: 0.6113  Val Loss: 0.6091\n",
      "Epoch 44/1000  Train Loss: 0.6092  Val Loss: 0.6096\n",
      "Epoch 45/1000  Train Loss: 0.6088  Val Loss: 0.6130\n",
      "Epoch 46/1000  Train Loss: 0.6088  Val Loss: 0.6111\n",
      "Epoch 47/1000  Train Loss: 0.6082  Val Loss: 0.6145\n",
      "Epoch 48/1000  Train Loss: 0.6134  Val Loss: 0.6148\n",
      "Epoch 49/1000  Train Loss: 0.6102  Val Loss: 0.6128\n",
      "Epoch 50/1000  Train Loss: 0.6095  Val Loss: 0.6087\n",
      "Epoch 51/1000  Train Loss: 0.6073  Val Loss: 0.6097\n",
      "Epoch 52/1000  Train Loss: 0.6064  Val Loss: 0.6133\n",
      "Epoch 53/1000  Train Loss: 0.6078  Val Loss: 0.6072\n",
      "Epoch 54/1000  Train Loss: 0.6049  Val Loss: 0.6080\n",
      "Epoch 55/1000  Train Loss: 0.6031  Val Loss: 0.6113\n",
      "Epoch 56/1000  Train Loss: 0.6051  Val Loss: 0.6124\n",
      "Epoch 57/1000  Train Loss: 0.6057  Val Loss: 0.6083\n",
      "Epoch 58/1000  Train Loss: 0.6039  Val Loss: 0.6093\n",
      "Epoch 59/1000  Train Loss: 0.6062  Val Loss: 0.6115\n",
      "Epoch 60/1000  Train Loss: 0.6050  Val Loss: 0.6130\n",
      "Epoch 61/1000  Train Loss: 0.6052  Val Loss: 0.6083\n",
      "Epoch 62/1000  Train Loss: 0.6061  Val Loss: 0.6102\n",
      "Epoch 63/1000  Train Loss: 0.6036  Val Loss: 0.6055\n",
      "Epoch 64/1000  Train Loss: 0.6048  Val Loss: 0.6059\n",
      "Epoch 65/1000  Train Loss: 0.6027  Val Loss: 0.6078\n",
      "Epoch 66/1000  Train Loss: 0.6041  Val Loss: 0.6057\n",
      "Epoch 67/1000  Train Loss: 0.6037  Val Loss: 0.6091\n",
      "Epoch 68/1000  Train Loss: 0.6024  Val Loss: 0.6063\n",
      "Epoch 69/1000  Train Loss: 0.6039  Val Loss: 0.6058\n",
      "Epoch 70/1000  Train Loss: 0.6026  Val Loss: 0.6075\n",
      "Epoch 71/1000  Train Loss: 0.6015  Val Loss: 0.6047\n",
      "Epoch 72/1000  Train Loss: 0.6023  Val Loss: 0.6063\n",
      "Epoch 73/1000  Train Loss: 0.6020  Val Loss: 0.6025\n",
      "Epoch 74/1000  Train Loss: 0.6008  Val Loss: 0.6040\n",
      "Epoch 75/1000  Train Loss: 0.6009  Val Loss: 0.6044\n",
      "Epoch 76/1000  Train Loss: 0.6005  Val Loss: 0.6057\n",
      "Epoch 77/1000  Train Loss: 0.6001  Val Loss: 0.6045\n",
      "Epoch 78/1000  Train Loss: 0.6001  Val Loss: 0.6070\n",
      "Epoch 79/1000  Train Loss: 0.6008  Val Loss: 0.6050\n",
      "Epoch 80/1000  Train Loss: 0.5998  Val Loss: 0.6052\n",
      "Epoch 81/1000  Train Loss: 0.6014  Val Loss: 0.6032\n",
      "Epoch 82/1000  Train Loss: 0.5996  Val Loss: 0.6068\n",
      "Epoch 83/1000  Train Loss: 0.6012  Val Loss: 0.6098\n",
      "Epoch 84/1000  Train Loss: 0.6018  Val Loss: 0.6052\n",
      "Epoch 85/1000  Train Loss: 0.5998  Val Loss: 0.6031\n",
      "Epoch 86/1000  Train Loss: 0.5994  Val Loss: 0.6036\n",
      "Epoch 87/1000  Train Loss: 0.6008  Val Loss: 0.6053\n",
      "Epoch 88/1000  Train Loss: 0.6011  Val Loss: 0.6040\n",
      "Early stopping at epoch 88 due to no improvement in validation loss.\n",
      "The training loop ran for 61.6 minutes for 88 epochs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 15 # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')  # Initialize best validation loss as a large number\n",
    "patience_counter = 0.0 # Counter for epochs with no improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)   # shape: (B, 62, 120, 120)\n",
    "        labels = labels.to(device)   # shape: (B, 120, 120)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)      # shape: (B, 3, 120, 120)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward(retain_graph=True)\n",
    "        # second backward pass on the same graph\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_val_loss = val_loss / len(val_dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  Train Loss: {epoch_loss:.4f}  Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0  # Reset counter if we have improvement\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "epochs_run = epoch + 1\n",
    "\n",
    "end = time.time()\n",
    "duration = (end-start)/60\n",
    "print(f\"The training loop ran for {round(duration,2)} minutes for {epochs_run} epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a1f11-ed09-4a2f-bb98-32bcefd46ecb",
   "metadata": {},
   "source": [
    "##### TRAINING LOSS AND VALIDATION LOSS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "287e9885-f0c1-4f46-ab96-94e819b69880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, epochs_run + 1))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a238a-6687-4180-b33c-830cb8bce86b",
   "metadata": {},
   "source": [
    "### MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e48ddf24-9b19-4eba-b4d5-418760a43350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) for each class.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      list: IoU for each class.\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    # Flatten the tensors for easier computation.\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        union = pred_inds.sum().float() + target_inds.sum().float() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # Alternatively, you might want to append 0.0\n",
    "        else:\n",
    "            ious.append((intersection / union).item())\n",
    "    return ious\n",
    "\n",
    "def compute_dice(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute the Dice coefficient for each class.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      list: Dice coefficient for each class.\n",
    "    \"\"\"\n",
    "    dices = []\n",
    "    # Flatten the tensors.\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        total = pred_inds.sum().float() + target_inds.sum().float()\n",
    "        if total == 0:\n",
    "            dices.append(float('nan'))\n",
    "        else:\n",
    "            dices.append((2 * intersection / total).item())\n",
    "    return dices\n",
    "\n",
    "def compute_confusion_matrix(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute a confusion matrix where the rows correspond to true classes\n",
    "    and the columns correspond to predicted classes.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      torch.Tensor: A (num_classes, num_classes) confusion matrix.\n",
    "    \"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    # Use torch.bincount to count occurrences of each (true, pred) pair.\n",
    "    cm = torch.bincount(num_classes * target + pred, minlength=num_classes**2)\n",
    "    cm = cm.reshape(num_classes, num_classes)\n",
    "    return cm\n",
    "\n",
    "def compute_precision_recall_f1(conf_matrix):\n",
    "    \"\"\"\n",
    "    Compute per-class precision, recall, and F1 score from a confusion matrix.\n",
    "    Args:\n",
    "      conf_matrix (torch.Tensor): A (num_classes, num_classes) confusion matrix.\n",
    "    Returns:\n",
    "      tuple: Three lists containing precision, recall, and F1 score for each class.\n",
    "    \"\"\"\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i].item()\n",
    "        FP = conf_matrix[:, i].sum().item() - TP\n",
    "        FN = conf_matrix[i, :].sum().item() - TP\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else float('nan')\n",
    "        recall    = TP / (TP + FN) if (TP + FN) > 0 else float('nan')\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else float('nan')\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    return precisions, recalls, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e628c978-92f3-41dd-93f0-3e4bfc9c426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6286\n",
      "torch.Size([366, 120, 120])\n",
      "torch.Size([366, 120, 120])\n",
      "IoU per class: [0.5768613815307617, 0.05130608752369881, 0.09173601120710373]\n",
      "Dice per class: [0.7316576838493347, 0.09760446846485138, 0.16805529594421387]\n",
      "Confusion Matrix:\n",
      " tensor([[2923560, 1113103, 1028212],\n",
      "        [   2649,   62125,   25119],\n",
      "        [    522,    7874,  107236]])\n",
      "Precision per class: [0.9989165386227843, 0.052510265387092574, 0.09239966326804054]\n",
      "Recall per class: [0.5772225375749648, 0.69109941819719, 0.9273903417739034]\n",
      "F1 per class: [0.731657691833156, 0.09760446820293874, 0.16805529545157144]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Convert raw logits to predicted class labels (shape: (batch_size, H, W))\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Collect predictions and ground truth labels for metric computations\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Calculate average test loss over the dataset\n",
    "test_loss = test_loss / len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Concatenate all the collected predictions and labels along the batch dimension\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "print(all_preds.shape)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "print(all_labels.shape)\n",
    "\n",
    "\n",
    "# Compute metrics using the functions defined earlier\n",
    "ious = compute_iou(all_preds, all_labels, num_classes=3)\n",
    "dices = compute_dice(all_preds, all_labels, num_classes=3)\n",
    "conf_matrix = compute_confusion_matrix(all_preds, all_labels, num_classes=3)\n",
    "precisions, recalls, f1s = compute_precision_recall_f1(conf_matrix)\n",
    "\n",
    "print(\"IoU per class:\", ious)\n",
    "print(\"Dice per class:\", dices)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Precision per class:\", precisions)\n",
    "print(\"Recall per class:\", recalls)\n",
    "print(\"F1 per class:\", f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d01a8fd5-6e6b-4431-9249-911b466f97d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Accuracy: 0.5868474841117859\n",
      "Confusion Matrix:\n",
      " [[2923560 1113103 1028212]\n",
      " [   2649   62125   25119]\n",
      " [    522    7874  107236]]\n",
      "Precision per class: [0.99891654 0.05251027 0.09239966]\n",
      "Recall per class: [0.57722254 0.69109942 0.92739034]\n",
      "F1 Score per class: [0.73165769 0.09760447 0.1680553 ]\n",
      "IoU per class: [0.5768613815307617, 0.05130608752369881, 0.09173601120710373]\n",
      "Dice Coefficient per class: [0.7316576838493347, 0.09760446846485138, 0.16805529594421387]\n",
      "Cohen's Kappa: 0.09729011054894798\n",
      "Balanced Accuracy: 0.7319040991820195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, balanced_accuracy_score\n",
    "\n",
    "# Example tensors (replace these with your actual data)\n",
    "# predictions = torch.randint(0, 3, (366, 120, 120))\n",
    "# true_labels = torch.randint(0, 3, (366, 120, 120))\n",
    "\n",
    "# Flatten the tensors so each pixel is one element\n",
    "pred_flat = all_preds.view(-1)\n",
    "true_flat = all_labels.view(-1)\n",
    "\n",
    "# 1. Pixel Accuracy\n",
    "pixel_accuracy = (pred_flat == true_flat).float().mean().item()\n",
    "print(\"Pixel Accuracy:\", pixel_accuracy)\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2])\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# 3. Precision, Recall, and F1 Score (per class)\n",
    "precision = precision_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "recall = recall_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "f1 = f1_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "print(\"Precision per class:\", precision)\n",
    "print(\"Recall per class:\", recall)\n",
    "print(\"F1 Score per class:\", f1)\n",
    "\n",
    "# 4. Intersection over Union (IoU) for each class\n",
    "def compute_iou(pred, true, num_classes=3):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        # Create masks for the current class\n",
    "        pred_inds = (pred == cls)\n",
    "        true_inds = (true == cls)\n",
    "        # Calculate intersection and union\n",
    "        intersection = (pred_inds & true_inds).sum().float()\n",
    "        union = (pred_inds | true_inds).sum().float()\n",
    "        if union.item() == 0:\n",
    "            ious.append(float('nan'))  # Avoid division by zero if class is not present in the true labels\n",
    "        else:\n",
    "            ious.append((intersection / union).item())\n",
    "    return ious\n",
    "\n",
    "ious = compute_iou(pred_flat, true_flat)\n",
    "print(\"IoU per class:\", ious)\n",
    "\n",
    "# 5. Dice Coefficient for each class\n",
    "def compute_dice(pred, true, num_classes=3, eps=1e-6):\n",
    "    dices = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        true_inds = (true == cls)\n",
    "        intersection = (pred_inds & true_inds).sum().float()\n",
    "        dice = (2 * intersection) / (pred_inds.sum() + true_inds.sum() + eps)\n",
    "        dices.append(dice.item())\n",
    "    return dices\n",
    "\n",
    "dices = compute_dice(pred_flat, true_flat)\n",
    "print(\"Dice Coefficient per class:\", dices)\n",
    "\n",
    "# 6. Cohen's Kappa\n",
    "kappa = cohen_kappa_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy())\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "\n",
    "# 7. Balanced Accuracy\n",
    "balanced_acc = balanced_accuracy_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy())\n",
    "print(\"Balanced Accuracy:\", balanced_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b65e2-a2f3-40df-a431-7ab18e4920a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "410d1956-ab93-414c-9bdd-7683dd0aeb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([366, 120, 120])\n",
      "torch.Size([366, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(all_preds.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbfcb038-74eb-4f3a-8401-d65987e590f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory to save the images\n",
    "# output_dir = 'PRED'\n",
    "# os.makedirs(output_dir, exist_ok=True) \n",
    "# # Define a color map: 0 = red, 1 = green, 2 = blue\n",
    "# color_map = {\n",
    "#     0: (0, 255, 0),  # \n",
    "#     1: (0, 0, 255),  # Green\n",
    "#     2: (225, 0, 0)   # Blue\n",
    "# }\n",
    "\n",
    "# # Create an RGB image for each array\n",
    "# for i in range(all_preds.shape[0]):\n",
    "#     # Create an empty RGB image\n",
    "#     rgb_image = np.zeros((120, 120, 3), dtype=np.uint8)\n",
    "    \n",
    "#     # Map values to colors\n",
    "#     for value, color in color_map.items():\n",
    "#         rgb_image[all_preds[i] == value] = color\n",
    "    \n",
    "#     # Convert to PIL image and save\n",
    "#     image = Image.fromarray(rgb_image)\n",
    "#     image.save(os.path.join(output_dir, f'image_{i:03d}.png'))\n",
    "\n",
    "# print(f\"Saved {all_preds.shape[0]} RGB images to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f384464-6c5c-44dd-be35-7564e80c9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory to save the images\n",
    "# output_dir = 'TRUE'\n",
    "# os.makedirs(output_dir, exist_ok=True) \n",
    "# # Define a color map: 0 = red, 1 = green, 2 = blue\n",
    "# color_map = {\n",
    "#     0: (0, 255, 0),  # \n",
    "#     1: (0, 0, 255),  # Green\n",
    "#     2: (225, 0, 0)   # Blue\n",
    "# }\n",
    "\n",
    "# # Create an RGB image for each array\n",
    "# for i in range(all_labels.shape[0]):\n",
    "#     # Create an empty RGB image\n",
    "#     rgb_image = np.zeros((120, 120, 3), dtype=np.uint8)\n",
    "    \n",
    "#     # Map values to colors\n",
    "#     for value, color in color_map.items():\n",
    "#         rgb_image[all_labels[i] == value] = color\n",
    "    \n",
    "#     # Convert to PIL image and save\n",
    "#     image = Image.fromarray(rgb_image)\n",
    "#     image.save(os.path.join(output_dir, f'image_{i:03d}.png'))\n",
    "\n",
    "# print(f\"Saved {all_labels.shape[0]} RGB images to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d165e6-71f6-4bd9-849e-50ceb2732714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
