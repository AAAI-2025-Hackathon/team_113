{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaad991-895a-4089-985b-4a5326161d32",
   "metadata": {},
   "source": [
    "# MODELING V3 FEB21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8f11c-83b3-45d3-80d1-a4e2cf8a491f",
   "metadata": {},
   "source": [
    "### LOADING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13a7736-a299-4871-9e8b-6c5b974d1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3568e-e2bb-4c02-a6a8-e0f6a05d77e7",
   "metadata": {},
   "source": [
    "### DEFINING NECESSARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f85494-1666-4466-9653-59e701291302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling_batch(matrices, new_shape):\n",
    "    original_batch, original_height, original_width = matrices.shape  \n",
    "    target_height, target_width = new_shape\n",
    "    pool_size = original_width // target_width\n",
    "    reshaped = matrices.reshape(original_batch, original_height, target_width, pool_size)\n",
    "    pooled = np.max(reshaped, axis=3)\n",
    "    return pooled\n",
    "\n",
    "def count_and_percentage(array):\n",
    "    flattened = array.flatten()\n",
    "    unique, counts = np.unique(flattened, return_counts=True)\n",
    "    total_elements = flattened.size\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    percentage_dict = {key: (value / total_elements) * 100 for key, value in count_dict.items()}\n",
    "    return count_dict, percentage_dict\n",
    "\n",
    "def resize_tensor_bilinear(tensor, target_size, mode='bilinear'):\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    resized = F.interpolate(tensor, size=target_size, mode=mode, align_corners=False)\n",
    "    return resized.squeeze(0)\n",
    "\n",
    "def resize_tensor_nearest(tensor, target_size, mode='nearest'):\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    resized = F.interpolate(tensor, size=target_size, mode=mode)\n",
    "    return resized.squeeze(0)\n",
    "\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    min_val = tensor.min(dim=-1, keepdim=True).values.min(dim=-2, keepdim=True).values\n",
    "    max_val = tensor.max(dim=-1, keepdim=True).values.max(dim=-2, keepdim=True).values\n",
    "    return ((tensor - min_val) / ((max_val - min_val) + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec3269-f6ef-4045-a742-95545ce77fa0",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c05aa-0609-4329-8fcd-f06758ef3d25",
   "metadata": {},
   "source": [
    "##### LOADING ELEVATION, VEGETATION, SOIL VARIABLE AND SOIL COMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1665e65-c33d-4318-94cd-3ba9644e22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPATIAL MASK\n",
    "spatial_mask = np.load(\"Spatial_Mask.npy\")\n",
    "\n",
    "#ELEVATION DATA\n",
    "elevation_data = np.load('elevation_matrix.npy') \n",
    "\n",
    "#VEGETATION DATA\n",
    "vegetation_data = np.load('NLCD2021_OR.npy') \n",
    "\n",
    "#SOIL VARIABLE DATA\n",
    "directory = \"ERA5_matrices\"  \n",
    "npy_files = sorted([f for f in os.listdir(directory) if f.endswith('.npy')])\n",
    "matrices = [np.load(os.path.join(directory, file)) for file in npy_files]\n",
    "data = np.stack(matrices, axis=0)  # Shape: (num_files, height, width)\n",
    "soil_variable_data = np.transpose(data, (1, 0, 2, 3)) \n",
    "\n",
    "#SOIL COMPOSITION DATA\n",
    "data_folder = \"SOLUS\"\n",
    "npy_files = [f for f in os.listdir(data_folder) if f.endswith('.npy')]\n",
    "num_files = len(npy_files)\n",
    "transformed_data_list = []\n",
    "pca = PCA(n_components=1)\n",
    "for file in npy_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    soil_data = np.load(file_path)\n",
    "    reshaped_data = soil_data.reshape(7, -1).T\n",
    "    principal_component = pca.fit_transform(reshaped_data)  \n",
    "    reduced_data = principal_component.reshape(1306, 464)\n",
    "    transformed_data_list.append(reduced_data)\n",
    "soil_composition_data = np.stack(transformed_data_list, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9d42d2-5a07-432c-b28d-07f6279d0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPATIAL MASK : (120, 120)\n",
      "ELEVATION DATA : (10812, 10812)\n",
      "VEGETATION DATA : (15, 4353, 1547)\n",
      "SOIL VARIABLE DATA : (1096, 28, 5, 5)\n",
      "SOIL COMPOSITION DATA : (18, 1306, 464)\n"
     ]
    }
   ],
   "source": [
    "print(f\"SPATIAL MASK : {spatial_mask.shape}\")\n",
    "print(f\"ELEVATION DATA : {elevation_data.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_data.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_data.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0b21-859b-4b23-b0b2-efc98b80b7f2",
   "metadata": {},
   "source": [
    "##### LOADING OUTPUT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43033c4-81aa-45fa-94c5-1671bdc2eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {0.0: 15118676, 1.0: 303439, 2.0: 360285}\n",
      "Percentages: {0.0: 95.79453061638281, 1.0: 1.9226416768045418, 2.0: 2.282827706812652}\n"
     ]
    }
   ],
   "source": [
    "label = np.load('landslide_labels_stored_sequentially_compressed.npz') \n",
    "output_labels = label['matrix']\n",
    "labels_resized = max_pooling_batch(output_labels, (120, 120))  # Output shape: (1096, 120, 120)\n",
    "counts, percentages = count_and_percentage(labels_resized)\n",
    "print(\"Counts:\", counts)  \n",
    "print(\"Percentages:\", percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1775a8cf-79fd-4a99-a6db-2cea7d2395ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT LABELS : (1096, 120, 120)\n"
     ]
    }
   ],
   "source": [
    "print(f\"OUTPUT LABELS : {labels_resized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc3b64-f9b0-49a1-abe1-ae23070081e6",
   "metadata": {},
   "source": [
    "### CONVERTING NUMPY ARRAYS TO TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a468bd-c653-4066-bef2-a51a29a7252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_data = torch.tensor(elevation_data, dtype=torch.float).unsqueeze(0)  # (1, 10812, 10812)\n",
    "vegetation_data = torch.tensor(vegetation_data, dtype=torch.float)             # (15, 4353, 1547)\n",
    "soil_composition_data = torch.tensor(soil_composition_data, dtype=torch.float) # (18, 1306, 464)\n",
    "soil_variable_data = torch.tensor(soil_variable_data, dtype=torch.float)       # (1096, 28, 5, 5)\n",
    "output_labels = torch.tensor(labels_resized, dtype=torch.long)                 # (1096, 120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7486f2a-7043-4e3e-ae15-fa03759ce9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVATION DATA : torch.Size([1, 10812, 10812])\n",
      "VEGETATION DATA : torch.Size([15, 4353, 1547])\n",
      "SOIL VARIABLE DATA : torch.Size([1096, 28, 5, 5])\n",
      "SOIL COMPOSITION DATA : torch.Size([18, 1306, 464])\n",
      "OUTPUT LABELS : torch.Size([1096, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ELEVATION DATA : {elevation_data.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_data.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_data.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_data.shape}\")\n",
    "print(f\"OUTPUT LABELS : {output_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec0fe9-4202-48be-8bdf-5c8115fde526",
   "metadata": {},
   "source": [
    "### PREPROCESSING THE TENSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644f2db-171d-4349-b379-28bd8bf4a2bf",
   "metadata": {},
   "source": [
    "##### TARGET SPATIAL RESOLUTION (HEIGHT,WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4234b43c-845b-4eee-bd66-e6a78355747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (120, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8e9b7-89fe-485c-acf1-666cc992c41c",
   "metadata": {},
   "source": [
    "##### PRE-RESIZE STATIC DATASETS (DO NOT CHANGE BY THE DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164e301d-43ac-4736-b6e2-5ebccd365eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_resized = resize_tensor_bilinear(elevation_data, target_size)                # -> (1, 120, 120)\n",
    "vegetation_resized = resize_tensor_nearest(vegetation_data, target_size)                # -> (15, 120, 120)\n",
    "soil_composition_resized = resize_tensor_bilinear(soil_composition_data, target_size)  # -> (18, 120, 120)\n",
    "#soil_variable_resized = F.interpolate(soil_variable_data, size=target_size, mode='bilinear', align_corners=False)  # -> (1096, 28, 120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41682232-2ce2-4c7a-9cd6-898b4d9834a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVATION DATA : torch.Size([1, 120, 120])\n",
      "VEGETATION DATA : torch.Size([15, 120, 120])\n",
      "SOIL COMPOSITION DATA : torch.Size([18, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ELEVATION DATA : {elevation_resized.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_resized.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_resized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b99adb-9fb1-4f7b-ad66-32c66fd7c115",
   "metadata": {},
   "source": [
    "##### NORMALIZE THE CONTINOUS FEATURES (ELEVATION AND SOIL COMPOSITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20bcbf75-a339-4d1d-a590-890dbcfaa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_normalized = (elevation_resized - elevation_resized.min()) / (elevation_resized.max() - elevation_resized.min())\n",
    "soil_composition_normalized = torch.stack([normalize_tensor(t1) for t1 in soil_composition_resized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd907e4-5eb6-46e5-9850-9bdf2f8f1a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVATION DATA : torch.Size([1, 120, 120])\n",
      "VEGETATION DATA : torch.Size([15, 120, 120])\n",
      "SOIL COMPOSITION DATA : torch.Size([18, 120, 120])\n",
      "SOIL VARIABLE DATA : torch.Size([1096, 28, 5, 5])\n",
      "OUTPUT LABELS : torch.Size([1096, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ELEVATION DATA : {elevation_normalized.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_resized.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_normalized.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_data.shape}\")\n",
    "print(f\"OUTPUT LABELS : {output_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb81b3f-de3e-42ed-bf65-1a266fe6625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "print(torch.isnan(elevation_normalized).any())\n",
    "print(torch.isnan(vegetation_resized).any())\n",
    "print(torch.isnan(soil_composition_normalized).any())\n",
    "print(torch.isnan(soil_variable_data).any())\n",
    "print(torch.isnan(output_labels).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405b845-1859-458e-8a82-607a3cbbd1f0",
   "metadata": {},
   "source": [
    "### CUSTOM DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d769f491-8eed-465c-bd27-a579d6152368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DayDataset(Dataset):\n",
    "    def __init__(self, day_indices, vegetation, elevation, soil_comp, soil_var, labels, target_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          day_indices: list of day indices (e.g., [0, 1, 2, ...])\n",
    "          vegetation: static vegetation data, already resized, shape (15, target_H, target_W)\n",
    "          elevation: static elevation data, already resized, shape (1, target_H, target_W)\n",
    "          soil_comp: static soil composition data, already resized, shape (18, target_H, target_W)\n",
    "          soil_var: daily soil variable data, shape (1096, 28, 5, 5)\n",
    "          labels: daily output labels, shape (1096, target_H, target_W)\n",
    "          target_size: the target spatial size (target_H, target_W) for all inputs\n",
    "        \"\"\"\n",
    "        self.day_indices = day_indices\n",
    "        self.vegetation = vegetation\n",
    "        self.elevation = elevation\n",
    "        self.soil_comp = soil_comp\n",
    "        self.soil_var = soil_var\n",
    "        self.labels = labels\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.day_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day = self.day_indices[idx]\n",
    "        soil_var_day = self.soil_var[day] \n",
    "        soil_var_day_resized = resize_tensor_bilinear(soil_var_day, self.target_size)\n",
    "        soil_variable_normalized = torch.stack([normalize_tensor(t2) for t2 in soil_var_day_resized])\n",
    "        \n",
    "        \n",
    "        input_tensor = torch.cat([self.vegetation, self.elevation, self.soil_comp, soil_variable_normalized], dim=0)\n",
    "        label = self.labels[day]\n",
    "        #print(f\"INPUT TENSOR WITH MISSING VALUE : {torch.isnan(input_tensor).any()}\")\n",
    "        return input_tensor, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd3f3-a662-4635-ad46-ff63c137a3bf",
   "metadata": {},
   "source": [
    "##### DEFINING TRAIN DAYS, VALIDATION DAYS AND TEST DAYS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c88eb50e-6aa5-4b56-b856-6725135fa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_days = list(range(1096))\n",
    "# train_days = list(range(0, 730))\n",
    "# test_days = list(range(730, 1096))\n",
    "# val_days = train_days[-100:] \n",
    "# train_days = train_days[:-100]\n",
    "all_days = list(range(1096))\n",
    "train_days = list(range(0, 730))\n",
    "test_days = list(range(730, 1096))\n",
    "val_days = random.sample(train_days, 100) # Randomly select 100 days from train_days for validation\n",
    "train_days = [day for day in train_days if day not in val_days] # Remove the selected validation days from train_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fd7a0-2744-4422-8118-51a5b41f5a1b",
   "metadata": {},
   "source": [
    "##### CREATING DATASETS AND DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16d5d11f-9dee-410d-8421-61dc00951d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DayDataset(train_days, vegetation_resized, elevation_normalized,\n",
    "                           soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "val_dataset = DayDataset(val_days, vegetation_resized, elevation_normalized,\n",
    "                         soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "test_dataset = DayDataset(test_days, vegetation_resized, elevation_normalized,\n",
    "                          soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "\n",
    "\n",
    "# train_dataset = DayDataset(train_days, vegetation_resized, elevation_resized,\n",
    "#                            soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "# val_dataset = DayDataset(val_days, vegetation_resized, elevation_resized,\n",
    "#                          soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "# test_dataset = DayDataset(test_days, vegetation_resized, elevation_resized,\n",
    "#                           soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 16 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecac178-1a05-4820-9545-92dbf8c59c75",
   "metadata": {},
   "source": [
    "### DEFINING FULLY CONVOLUTIONAL NETWORK (FCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4505fdd5-ce9e-4326-af80-9cd0aad6ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shelly/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (enc1): Sequential(\n",
       "    (0): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottom): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec3): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec2): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec1): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Bottom (bottleneck)\n",
    "        self.bottom = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 256, 256, kernel_size=3, padding=1),  # Concatenated channels from enc3\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 128, 128, kernel_size=3, padding=1),  # Concatenated channels from enc2\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64 + 64, 64, kernel_size=3, padding=1),    # Concatenated channels from enc1\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.enc1(x)   # (64, H, W)\n",
    "        p1 = self.pool1(x1) # (64, H/2, W/2)\n",
    "\n",
    "        x2 = self.enc2(p1)  # (128, H/2, W/2)\n",
    "        p2 = self.pool2(x2) # (128, H/4, W/4)\n",
    "\n",
    "        x3 = self.enc3(p2)  # (256, H/4, W/4)\n",
    "        p3 = self.pool3(x3) # (256, H/8, W/8)\n",
    "\n",
    "        # Bottleneck\n",
    "        x_bottom = self.bottom(p3) # (512, H/8, W/8)\n",
    "\n",
    "        # Decoder\n",
    "        d3 = self.upconv3(x_bottom)        # (256, H/4, W/4)\n",
    "        d3 = torch.cat([d3, x3], dim=1)      # (256 + 256, H/4, W/4)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)              # (128, H/2, W/2)\n",
    "        d2 = torch.cat([d2, x2], dim=1)      # (128 + 128, H/2, W/2)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)              # (64, H, W)\n",
    "        d1 = torch.cat([d1, x1], dim=1)      # (64 + 64, H, W)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        out = self.final_conv(d1)          # (num_classes, H, W)\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "in_channels = 15 + 1 + 18 + 28  # As defined in your FCN\n",
    "num_classes = 3\n",
    "model = UNet(in_channels, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b224ae-a4b6-48f0-b294-8c551500fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FCN(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes):\n",
    "#         \"\"\"\n",
    "#         A simple encoder-decoder Fully Convolutional Network.\n",
    "#         Args:\n",
    "#           in_channels: number of input channels (here, 62)\n",
    "#           num_classes: number of output classes (here, 3 for labels 0, 1, 2)\n",
    "#         \"\"\"\n",
    "#         super(FCN, self).__init__()\n",
    "#         # Encoder: three conv layers with pooling\n",
    "#         self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "#         self.bn1   = nn.BatchNorm2d(64)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)  \n",
    "\n",
    "#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.bn2   = nn.BatchNorm2d(128)\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.bn3   = nn.BatchNorm2d(256)\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         # Decoder: upsampling via transpose convolutions\n",
    "#         self.upconv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.upconv3 = nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         x = F.relu(self.bn1(self.conv1(x)))    # (64, H, W)\n",
    "#         x = self.pool1(x)                      # (64, H/2, W/2)\n",
    "\n",
    "#         x = F.relu(self.bn2(self.conv2(x)))    # (128, H/2, W/2)\n",
    "#         x = self.pool2(x)                      # (128, H/4, W/4)\n",
    "\n",
    "#         x = F.relu(self.bn3(self.conv3(x)))    # (256, H/4, W/4)\n",
    "#         x = self.pool3(x)                      # (256, H/8, W/8)\n",
    "\n",
    "#         # Decoder (upsample back to original resolution)\n",
    "#         x = self.upconv1(x)                    # (128, H/4, W/4)\n",
    "#         x = self.upconv2(x)                    # (64, H/2, W/2)\n",
    "#         x = self.upconv3(x)                    # (num_classes, H, W)\n",
    "#         return x\n",
    "\n",
    "# in_channels = 15 + 1 + 18 + 28  \n",
    "# num_classes = 3\n",
    "# model = FCN(in_channels, num_classes)\n",
    "# #model = DeepResMultiScaleFCN(in_channels, num_classes)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c6195-e61a-4fe4-90f7-4d635ff33609",
   "metadata": {},
   "source": [
    "### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e6451-a130-45e2-ad77-ec213514a3ef",
   "metadata": {},
   "source": [
    "##### DEFINING EARLY STOPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08b2233f-304f-4445-a228-af7bfe8aa226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68e818-7b1d-47a8-8593-34fd6f9006e6",
   "metadata": {},
   "source": [
    "##### DEFINING THE LOSS FUNCTION AND OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "423e86cc-eda3-4ab1-bc06-1c91b15b4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = torch.tensor([0.9579453061638281, 0.019226416768045418, 0.02282827706812652])\n",
    "weights = 1.0 / freq\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)  \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "early_stopping = EarlyStopping(patience=15, delta=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89e926-a27b-446e-baff-9c1f34bab32d",
   "metadata": {},
   "source": [
    "##### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5743f745-4d24-4478-a1fd-78e0cfd31e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# num_epochs = 1000\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs = inputs.to(device)   # shape: (B, 62, 120, 120)\n",
    "#         labels = labels.to(device)   # shape: (B, 120, 120)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)      # shape: (B, 3, 120, 120)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#     epoch_loss = running_loss / len(train_dataset)\n",
    "#     train_losses.append(epoch_loss)\n",
    "\n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)V2\n",
    "#             val_loss += loss.item() * inputs.size(0)\n",
    "#     epoch_val_loss = val_loss / len(val_dataset)\n",
    "#     val_losses.append(epoch_val_loss)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}  Train Loss: {epoch_loss:.4f}  Val Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "#     early_stopping(val_loss, model)\n",
    "#     if early_stopping.early_stop:\n",
    "#         print(\"Early stopping\")\n",
    "#         break\n",
    "\n",
    "# early_stopping.load_best_model(model)\n",
    "\n",
    "# epochs_run = epoch + 1\n",
    "\n",
    "# end = time.time()\n",
    "# duration = (end-start)/60\n",
    "# print(f\"The training Loop ran for {round(duration,2)} minutes for {num_epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3be989f1-8dc4-4fc7-a9bf-c3b931101278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000  Train Loss: 1.0144  Val Loss: 0.9631\n",
      "Epoch 2/1000  Train Loss: 0.8630  Val Loss: 0.8311\n",
      "Epoch 3/1000  Train Loss: 0.7703  Val Loss: 0.7577\n",
      "Epoch 4/1000  Train Loss: 0.6975  Val Loss: 0.7236\n",
      "Epoch 5/1000  Train Loss: 0.6800  Val Loss: 0.7545\n",
      "Epoch 6/1000  Train Loss: 0.6643  Val Loss: 0.6818\n",
      "Epoch 7/1000  Train Loss: 0.6432  Val Loss: 0.6719\n",
      "Epoch 8/1000  Train Loss: 0.6358  Val Loss: 0.6669\n",
      "Epoch 9/1000  Train Loss: 0.6201  Val Loss: 0.6726\n",
      "Epoch 10/1000  Train Loss: 0.6188  Val Loss: 0.6556\n",
      "Epoch 11/1000  Train Loss: 0.6113  Val Loss: 0.6376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)   \u001b[38;5;66;03m# shape: (B, 120, 120)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# shape: (B, 3, 120, 120)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 82\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv2(d3)              \u001b[38;5;66;03m# (128, H/2, W/2)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m d2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([d2, x2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)      \u001b[38;5;66;03m# (128 + 128, H/2, W/2)\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m d2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec2\u001b[49m\u001b[43m(\u001b[49m\u001b[43md2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupconv1(d2)              \u001b[38;5;66;03m# (64, H, W)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m d1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([d1, x1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)      \u001b[38;5;66;03m# (64 + 64, H, W)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 15 # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')  # Initialize best validation loss as a large number\n",
    "patience_counter = 0.0 # Counter for epochs with no improvement\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)   # shape: (B, 62, 120, 120)\n",
    "        labels = labels.to(device)   # shape: (B, 120, 120)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)      # shape: (B, 3, 120, 120)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_val_loss = val_loss / len(val_dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  Train Loss: {epoch_loss:.4f}  Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0  # Reset counter if we have improvement\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "epochs_run = epoch + 1\n",
    "\n",
    "end = time.time()\n",
    "duration = (end-start)/60\n",
    "print(f\"The training loop ran for {round(duration,2)} minutes for {epochs_run} epochs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a1f11-ed09-4a2f-bb98-32bcefd46ecb",
   "metadata": {},
   "source": [
    "##### TRAINING LOSS AND VALIDATION LOSS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e9885-f0c1-4f46-ab96-94e819b69880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, epochs_run + 1))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a238a-6687-4180-b33c-830cb8bce86b",
   "metadata": {},
   "source": [
    "### MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ddf24-9b19-4eba-b4d5-418760a43350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) for each class.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      list: IoU for each class.\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    # Flatten the tensors for easier computation.\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        union = pred_inds.sum().float() + target_inds.sum().float() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # Alternatively, you might want to append 0.0\n",
    "        else:\n",
    "            ious.append((intersection / union).item())\n",
    "    return ious\n",
    "\n",
    "def compute_dice(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute the Dice coefficient for each class.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      list: Dice coefficient for each class.\n",
    "    \"\"\"\n",
    "    dices = []\n",
    "    # Flatten the tensors.\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        total = pred_inds.sum().float() + target_inds.sum().float()\n",
    "        if total == 0:\n",
    "            dices.append(float('nan'))\n",
    "        else:\n",
    "            dices.append((2 * intersection / total).item())\n",
    "    return dices\n",
    "\n",
    "def compute_confusion_matrix(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute a confusion matrix where the rows correspond to true classes\n",
    "    and the columns correspond to predicted classes.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      torch.Tensor: A (num_classes, num_classes) confusion matrix.\n",
    "    \"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    # Use torch.bincount to count occurrences of each (true, pred) pair.\n",
    "    cm = torch.bincount(num_classes * target + pred, minlength=num_classes**2)\n",
    "    cm = cm.reshape(num_classes, num_classes)\n",
    "    return cm\n",
    "\n",
    "def compute_precision_recall_f1(conf_matrix):\n",
    "    \"\"\"\n",
    "    Compute per-class precision, recall, and F1 score from a confusion matrix.\n",
    "    Args:\n",
    "      conf_matrix (torch.Tensor): A (num_classes, num_classes) confusion matrix.\n",
    "    Returns:\n",
    "      tuple: Three lists containing precision, recall, and F1 score for each class.\n",
    "    \"\"\"\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i].item()\n",
    "        FP = conf_matrix[:, i].sum().item() - TP\n",
    "        FN = conf_matrix[i, :].sum().item() - TP\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else float('nan')\n",
    "        recall    = TP / (TP + FN) if (TP + FN) > 0 else float('nan')\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else float('nan')\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    return precisions, recalls, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628c978-92f3-41dd-93f0-3e4bfc9c426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Convert raw logits to predicted class labels (shape: (batch_size, H, W))\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Collect predictions and ground truth labels for metric computations\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Calculate average test loss over the dataset\n",
    "test_loss = test_loss / len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Concatenate all the collected predictions and labels along the batch dimension\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "print(all_preds.shape)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "print(all_labels.shape)\n",
    "\n",
    "\n",
    "# Compute metrics using the functions defined earlier\n",
    "ious = compute_iou(all_preds, all_labels, num_classes=3)\n",
    "dices = compute_dice(all_preds, all_labels, num_classes=3)\n",
    "conf_matrix = compute_confusion_matrix(all_preds, all_labels, num_classes=3)\n",
    "precisions, recalls, f1s = compute_precision_recall_f1(conf_matrix)\n",
    "\n",
    "print(\"IoU per class:\", ious)\n",
    "print(\"Dice per class:\", dices)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Precision per class:\", precisions)\n",
    "print(\"Recall per class:\", recalls)\n",
    "print(\"F1 per class:\", f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a8fd5-6e6b-4431-9249-911b466f97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, balanced_accuracy_score\n",
    "\n",
    "# Example tensors (replace these with your actual data)\n",
    "# predictions = torch.randint(0, 3, (366, 120, 120))\n",
    "# true_labels = torch.randint(0, 3, (366, 120, 120))\n",
    "\n",
    "# Flatten the tensors so each pixel is one element\n",
    "pred_flat = all_preds.view(-1)\n",
    "true_flat = all_labels.view(-1)\n",
    "\n",
    "# 1. Pixel Accuracy\n",
    "pixel_accuracy = (pred_flat == true_flat).float().mean().item()\n",
    "print(\"Pixel Accuracy:\", pixel_accuracy)\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2])\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# 3. Precision, Recall, and F1 Score (per class)\n",
    "precision = precision_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "recall = recall_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "f1 = f1_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "print(\"Precision per class:\", precision)\n",
    "print(\"Recall per class:\", recall)\n",
    "print(\"F1 Score per class:\", f1)\n",
    "\n",
    "# 4. Intersection over Union (IoU) for each class\n",
    "def compute_iou(pred, true, num_classes=3):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        # Create masks for the current class\n",
    "        pred_inds = (pred == cls)\n",
    "        true_inds = (true == cls)\n",
    "        # Calculate intersection and union\n",
    "        intersection = (pred_inds & true_inds).sum().float()\n",
    "        union = (pred_inds | true_inds).sum().float()\n",
    "        if union.item() == 0:\n",
    "            ious.append(float('nan'))  # Avoid division by zero if class is not present in the true labels\n",
    "        else:\n",
    "            ious.append((intersection / union).item())\n",
    "    return ious\n",
    "\n",
    "ious = compute_iou(pred_flat, true_flat)\n",
    "print(\"IoU per class:\", ious)\n",
    "\n",
    "# 5. Dice Coefficient for each class\n",
    "def compute_dice(pred, true, num_classes=3, eps=1e-6):\n",
    "    dices = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        true_inds = (true == cls)\n",
    "        intersection = (pred_inds & true_inds).sum().float()\n",
    "        dice = (2 * intersection) / (pred_inds.sum() + true_inds.sum() + eps)\n",
    "        dices.append(dice.item())\n",
    "    return dices\n",
    "\n",
    "dices = compute_dice(pred_flat, true_flat)\n",
    "print(\"Dice Coefficient per class:\", dices)\n",
    "\n",
    "# 6. Cohen's Kappa\n",
    "kappa = cohen_kappa_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy())\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "\n",
    "# 7. Balanced Accuracy\n",
    "balanced_acc = balanced_accuracy_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy())\n",
    "print(\"Balanced Accuracy:\", balanced_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b65e2-a2f3-40df-a431-7ab18e4920a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d1956-ab93-414c-9bdd-7683dd0aeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_preds.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee17ecb-bf52-44dc-b3d5-bdb328ba4ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def iou(pred, target):\n",
    "    intersection = torch.sum(pred * target)\n",
    "    union = torch.sum(pred) + torch.sum(target) - intersection\n",
    "    return intersection / union\n",
    "\n",
    "def dice(pred, target):\n",
    "    intersection = torch.sum(pred * target)\n",
    "    return (2 * intersection) / (torch.sum(pred) + torch.sum(target))\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    correct = torch.sum(pred == target)\n",
    "    total = target.numel()  # Total number of elements\n",
    "    return correct / total\n",
    "\n",
    "def f1_score(pred, target):\n",
    "    tp = torch.sum(pred * target)\n",
    "    fp = torch.sum(pred * (1 - target))\n",
    "    fn = torch.sum((1 - pred) * target)\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "def macro_f1(pred_matrices, ground_truth_matrices):\n",
    "    f1_scores = []\n",
    "    for day in range(366):\n",
    "        pred_matrix = pred_matrices[day]\n",
    "        ground_truth_matrix = ground_truth_matrices[day]\n",
    "        f1 = f1_score(pred_matrix, ground_truth_matrix)\n",
    "        f1_scores.append(f1.item())\n",
    "    \n",
    "    return sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "       # Ground truth matrices (binary)\n",
    "\n",
    "# Assuming `predicted_matrices` and `ground_truth_matrices` are your 366 x 120 x 120 tensors\n",
    "for day in range(366):\n",
    "    pred_matrix = all_preds[day]  # 120x120 for a given day\n",
    "    ground_truth_matrix = all_labels[day]  # 120x120 for a given day\n",
    "    iou_score = iou(pred_matrix, ground_truth_matrix)\n",
    "    dice_score = dice(pred_matrix, ground_truth_matrix)\n",
    "    acc_score = accuracy(pred_matrix, ground_truth_matrix)\n",
    "    f1 = f1_score(pred_matrix, ground_truth_matrix)\n",
    "    print(f\"Day {day + 1} IoU: {iou_score.item()}, DICE: {dice_score.item()}, Accuracy: {acc_score.item()}, F1: {f1.item()}\")\n",
    "\n",
    "# Calculate macro-F1 score across all days\n",
    "macro_f1_score = macro_f1(predicted_matrices, ground_truth_matrices)\n",
    "print(f\"Macro F1: {macro_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa811b1-86c7-4063-94b2-3854be99a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_metrics(pred, target):\n",
    "    \"\"\"\n",
    "    Compute overall metrics: IoU, Dice Coefficient, Accuracy, F1-score, and Macro-F1.\n",
    "    Args:\n",
    "        pred (torch.Tensor): Tensor of shape (366, 120, 120) - predicted binary matrices.\n",
    "        target (torch.Tensor): Tensor of shape (366, 120, 120) - ground truth binary matrices.\n",
    "    Returns:\n",
    "        dict: Dictionary with the computed metrics.\n",
    "    \"\"\"\n",
    "    # Flatten all matrices to a single 1D tensor\n",
    "    pred = pred.view(-1)  # Shape: (366 * 120 * 120,)\n",
    "    target = target.view(-1)  # Shape: (366 * 120 * 120,)\n",
    "\n",
    "    # Compute TP, FP, FN, TN\n",
    "    tp = torch.sum(pred * target)  # True Positives\n",
    "    fp = torch.sum(pred * (1 - target))  # False Positives\n",
    "    fn = torch.sum((1 - pred) * target)  # False Negatives\n",
    "    tn = torch.sum((1 - pred) * (1 - target))  # True Negatives\n",
    "\n",
    "    # IoU (Intersection over Union)\n",
    "    iou = tp / (tp + fp + fn + 1e-6)  # Adding epsilon to avoid division by zero\n",
    "\n",
    "    # Dice Coefficient\n",
    "    dice = (2 * tp) / (2 * tp + fp + fn + 1e-6)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "\n",
    "    # Precision and Recall for F1 Score\n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "\n",
    "    # Macro-F1 (by averaging F1 per day)\n",
    "    f1_per_day = []\n",
    "    for day in range(366):\n",
    "        day_pred = pred[day * 120 * 120: (day + 1) * 120 * 120]\n",
    "        day_target = target[day * 120 * 120: (day + 1) * 120 * 120]\n",
    "\n",
    "        day_tp = torch.sum(day_pred * day_target)\n",
    "        day_fp = torch.sum(day_pred * (1 - day_target))\n",
    "        day_fn = torch.sum((1 - day_pred) * day_target)\n",
    "\n",
    "        day_precision = day_tp / (day_tp + day_fp + 1e-6)\n",
    "        day_recall = day_tp / (day_tp + day_fn + 1e-6)\n",
    "        day_f1 = 2 * (day_precision * day_recall) / (day_precision + day_recall + 1e-6)\n",
    "\n",
    "        f1_per_day.append(day_f1.item())\n",
    "\n",
    "    macro_f1 = sum(f1_per_day) / len(f1_per_day)\n",
    "\n",
    "    return {\n",
    "        \"IoU\": iou.item(),\n",
    "        \"Dice Coefficient\": dice.item(),\n",
    "        \"Accuracy\": accuracy.item(),\n",
    "        \"F1 Score\": f1.item(),\n",
    "        \"Macro F1\": macro_f1\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Assuming predicted_matrices and ground_truth_matrices are torch tensors of shape (366, 120, 120)\n",
    "print(all_preds.shape)\n",
    "print(all_labels.shape)\n",
    "\n",
    "metrics = compute_metrics(all_preds, all_labels)\n",
    "\n",
    "# Print results\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187a000-f242-4719-8246-4c5125bdccbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b14379-7866-4038-a7a2-a2b86f54773d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb209e9-f13f-4ef9-8739-9d44b63b1109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b958e5-2c3c-4e26-b682-9bbb8dfe4053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
