{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaad991-895a-4089-985b-4a5326161d32",
   "metadata": {},
   "source": [
    "# MODELING V3 FEB21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8f11c-83b3-45d3-80d1-a4e2cf8a491f",
   "metadata": {},
   "source": [
    "### LOADING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13a7736-a299-4871-9e8b-6c5b974d1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3568e-e2bb-4c02-a6a8-e0f6a05d77e7",
   "metadata": {},
   "source": [
    "### DEFINING NECESSARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f85494-1666-4466-9653-59e701291302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling_batch(matrices, new_shape):\n",
    "    original_batch, original_height, original_width = matrices.shape  \n",
    "    target_height, target_width = new_shape\n",
    "    pool_size = original_width // target_width\n",
    "    reshaped = matrices.reshape(original_batch, original_height, target_width, pool_size)\n",
    "    pooled = np.max(reshaped, axis=3)\n",
    "    return pooled\n",
    "\n",
    "def count_and_percentage(array):\n",
    "    flattened = array.flatten()\n",
    "    unique, counts = np.unique(flattened, return_counts=True)\n",
    "    total_elements = flattened.size\n",
    "    count_dict = dict(zip(unique, counts))\n",
    "    percentage_dict = {key: (value / total_elements) * 100 for key, value in count_dict.items()}\n",
    "    return count_dict, percentage_dict\n",
    "\n",
    "def resize_tensor_bilinear(tensor, target_size, mode='bilinear'):\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    resized = F.interpolate(tensor, size=target_size, mode=mode, align_corners=False)\n",
    "    return resized.squeeze(0)\n",
    "\n",
    "def resize_tensor_nearest(tensor, target_size, mode='nearest'):\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    resized = F.interpolate(tensor, size=target_size, mode=mode)\n",
    "    return resized.squeeze(0)\n",
    "\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    min_val = tensor.min(dim=-1, keepdim=True).values.min(dim=-2, keepdim=True).values\n",
    "    max_val = tensor.max(dim=-1, keepdim=True).values.max(dim=-2, keepdim=True).values\n",
    "    return ((tensor - min_val) / ((max_val - min_val) + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec3269-f6ef-4045-a742-95545ce77fa0",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c05aa-0609-4329-8fcd-f06758ef3d25",
   "metadata": {},
   "source": [
    "##### LOADING ELEVATION, VEGETATION, SOIL VARIABLE AND SOIL COMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1665e65-c33d-4318-94cd-3ba9644e22c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anylithicdpt_2D_matrix.npy\n",
      "resdept_2D_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "#ELEVATION DATA\n",
    "elevation_data = np.load('elevation_matrix.npy') \n",
    "\n",
    "#VEGETATION DATA\n",
    "vegetation_data = np.load('NLCD2021_OR.npy') \n",
    "\n",
    "#SOIL VARIABLE DATA\n",
    "directory = \"ERA5_matrices\"  \n",
    "npy_files = sorted([f for f in os.listdir(directory) if f.endswith('.npy')])\n",
    "matrices = [np.load(os.path.join(directory, file)) for file in npy_files]\n",
    "data = np.stack(matrices, axis=0)  # Shape: (num_files, height, width)\n",
    "soil_variable_data = np.transpose(data, (1, 0, 2, 3)) \n",
    "\n",
    "#SOIL COMPOSITION DATA\n",
    "data_folder = \"SOLUS\"\n",
    "npy_files = [f for f in os.listdir(data_folder) if f.endswith('.npy')]\n",
    "num_files = len(npy_files)\n",
    "transformed_data_list = []\n",
    "pca = PCA(n_components=1)\n",
    "for file in npy_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    soil_data = np.load(file_path)\n",
    "    try:\n",
    "        reshaped_data = soil_data.reshape(7, -1).T\n",
    "    except:\n",
    "        print(file)\n",
    "        continue\n",
    "    principal_component = pca.fit_transform(reshaped_data)  \n",
    "    reduced_data = principal_component.reshape(1306, 464)\n",
    "    transformed_data_list.append(reduced_data)\n",
    "soil_composition_data = np.stack(transformed_data_list, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9d42d2-5a07-432c-b28d-07f6279d0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVATION DATA : (10812, 10812)\n",
      "VEGETATION DATA : (15, 4353, 1547)\n",
      "SOIL VARIABLE DATA : (1096, 28, 5, 5)\n",
      "SOIL COMPOSITION DATA : (18, 1306, 464)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ELEVATION DATA : {elevation_data.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_data.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_data.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0b21-859b-4b23-b0b2-efc98b80b7f2",
   "metadata": {},
   "source": [
    "##### LOADING OUTPUT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43033c4-81aa-45fa-94c5-1671bdc2eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {0.0: 15118676, 1.0: 303439, 2.0: 360285}\n",
      "Percentages: {0.0: 95.79453061638281, 1.0: 1.9226416768045418, 2.0: 2.282827706812652}\n"
     ]
    }
   ],
   "source": [
    "label = np.load('landslide_labels_stored_sequentially_compressed.npz') \n",
    "output_labels = label['matrix']\n",
    "labels_resized = max_pooling_batch(output_labels, (120, 120))  # Output shape: (1096, 120, 120)\n",
    "counts, percentages = count_and_percentage(labels_resized)\n",
    "print(\"Counts:\", counts)  \n",
    "print(\"Percentages:\", percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1775a8cf-79fd-4a99-a6db-2cea7d2395ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT LABELS : (1096, 120, 120)\n"
     ]
    }
   ],
   "source": [
    "print(f\"OUTPUT LABELS : {labels_resized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc3b64-f9b0-49a1-abe1-ae23070081e6",
   "metadata": {},
   "source": [
    "### CONVERTING NUMPY ARRAYS TO TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a468bd-c653-4066-bef2-a51a29a7252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_data = torch.tensor(elevation_data, dtype=torch.float).unsqueeze(0)  # (1, 10812, 10812)\n",
    "vegetation_data = torch.tensor(vegetation_data, dtype=torch.float)             # (15, 4353, 1547)\n",
    "soil_composition_data = torch.tensor(soil_composition_data, dtype=torch.float) # (18, 1306, 464)\n",
    "soil_variable_data = torch.tensor(soil_variable_data, dtype=torch.float)       # (1096, 28, 5, 5)\n",
    "output_labels = torch.tensor(labels_resized, dtype=torch.long)                 # (1096, 120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7486f2a-7043-4e3e-ae15-fa03759ce9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVATION DATA : torch.Size([1, 10812, 10812])\n",
      "VEGETATION DATA : torch.Size([15, 4353, 1547])\n",
      "SOIL VARIABLE DATA : torch.Size([1096, 28, 5, 5])\n",
      "SOIL COMPOSITION DATA : torch.Size([18, 1306, 464])\n",
      "OUTPUT LABELS : torch.Size([1096, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "print(f\"ELEVATION DATA : {elevation_data.shape}\")\n",
    "print(f\"VEGETATION DATA : {vegetation_data.shape}\")\n",
    "print(f\"SOIL VARIABLE DATA : {soil_variable_data.shape}\")\n",
    "print(f\"SOIL COMPOSITION DATA : {soil_composition_data.shape}\")\n",
    "print(f\"OUTPUT LABELS : {output_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec0fe9-4202-48be-8bdf-5c8115fde526",
   "metadata": {},
   "source": [
    "### PREPROCESSING THE TENSORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644f2db-171d-4349-b379-28bd8bf4a2bf",
   "metadata": {},
   "source": [
    "##### TARGET SPATIAL RESOLUTION (HEIGHT,WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4234b43c-845b-4eee-bd66-e6a78355747d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (120, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bcbf75-a339-4d1d-a590-890dbcfaa734",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_normalized = (elevation_data - elevation_data.min()) / (elevation_data.max() - elevation_data.min())\n",
    "soil_composition_normalized = torch.stack([normalize_tensor(t1) for t1 in soil_composition_data])\n",
    "soil_variable_normalized = torch.stack([normalize_tensor(t2) for t2 in soil_variable_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b99adb-9fb1-4f7b-ad66-32c66fd7c115",
   "metadata": {},
   "source": [
    "##### NORMALIZE THE CONTINOUS FEATURES (ELEVATION AND SOIL COMPOSITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb81b3f-de3e-42ed-bf65-1a266fe6625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "print(torch.isnan(elevation_normalized).any())\n",
    "print(torch.isnan(vegetation_data).any())\n",
    "print(torch.isnan(soil_composition_normalized).any())\n",
    "print(torch.isnan(soil_variable_data).any())\n",
    "print(torch.isnan(output_labels).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405b845-1859-458e-8a82-607a3cbbd1f0",
   "metadata": {},
   "source": [
    "### CUSTOM DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d769f491-8eed-465c-bd27-a579d6152368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DayDataset(Dataset):\n",
    "    def __init__(self, day_indices, vegetation, elevation, soil_comp, soil_var, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          day_indices: list of day indices (e.g., [0, 1, 2, ...])\n",
    "          vegetation: static vegetation data, already resized, shape (15, target_H, target_W)\n",
    "          elevation: static elevation data, already resized, shape (1, target_H, target_W)\n",
    "          soil_comp: static soil composition data, already resized, shape (18, target_H, target_W)\n",
    "          soil_var: daily soil variable data, shape (1096, 28, 5, 5)\n",
    "          labels: daily output labels, shape (1096, target_H, target_W)\n",
    "          target_size: the target spatial size (target_H, target_W) for all inputs\n",
    "        \"\"\"\n",
    "        self.day_indices = day_indices\n",
    "        self.vegetation = vegetation\n",
    "        self.elevation = elevation\n",
    "        self.soil_comp = soil_comp\n",
    "        self.soil_var = soil_var\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.day_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day = self.day_indices[idx]\n",
    "        soil_var_day = self.soil_var[day] \n",
    "        #input_tensor = torch.cat([self.vegetation, self.elevation, self.soil_comp, soil_variable_normalized], dim=0)\n",
    "        label = self.labels[day]\n",
    "        \n",
    "        return self.vegetation, self.elevation, self.soil_comp, soil_var_day, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd3f3-a662-4635-ad46-ff63c137a3bf",
   "metadata": {},
   "source": [
    "##### DEFINING TRAIN DAYS, VALIDATION DAYS AND TEST DAYS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88eb50e-6aa5-4b56-b856-6725135fa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_days = list(range(1096))\n",
    "# train_days = list(range(0, 730))\n",
    "# test_days = list(range(730, 1096))\n",
    "# val_days = train_days[-100:] \n",
    "# train_days = train_days[:-100]\n",
    "all_days = list(range(1096))\n",
    "train_days = list(range(0, 730))\n",
    "test_days = list(range(730, 1096))\n",
    "val_days = random.sample(train_days, 100) # Randomly select 100 days from train_days for validation\n",
    "train_days = [day for day in train_days if day not in val_days] # Remove the selected validation days from train_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fd7a0-2744-4422-8118-51a5b41f5a1b",
   "metadata": {},
   "source": [
    "##### CREATING DATASETS AND DATALOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d5d11f-9dee-410d-8421-61dc00951d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DayDataset(train_days, vegetation_resized, elevation_normalized,\n",
    "#                            soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "# val_dataset = DayDataset(val_days, vegetation_resized, elevation_normalized,\n",
    "#                          soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "# test_dataset = DayDataset(test_days, vegetation_resized, elevation_normalized,\n",
    "#                           soil_composition_normalized, soil_variable_data, output_labels, target_size)\n",
    "\n",
    "\n",
    "train_dataset = DayDataset(train_days, vegetation_data, elevation_normalized,\n",
    "                           soil_composition_normalized, soil_variable_normalized, output_labels)\n",
    "val_dataset = DayDataset(val_days, vegetation_data, elevation_normalized,\n",
    "                           soil_composition_normalized, soil_variable_normalized, output_labels)\n",
    "test_dataset = DayDataset(test_days, vegetation_data, elevation_normalized,\n",
    "                           soil_composition_normalized, soil_variable_normalized, output_labels)\n",
    "\n",
    "\n",
    "# train_dataset = DayDataset(train_days, vegetation_resized, elevation_resized,\n",
    "#                            soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "# val_dataset = DayDataset(val_days, vegetation_resized, elevation_resized,\n",
    "#                          soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "# test_dataset = DayDataset(test_days, vegetation_resized, elevation_resized,\n",
    "#                           soil_composition_resized, soil_variable_data, output_labels, target_size)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 24\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecac178-1a05-4820-9545-92dbf8c59c75",
   "metadata": {},
   "source": [
    "### DEFINING FULLY CONVOLUTIONAL NETWORK (FCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4505fdd5-ce9e-4326-af80-9cd0aad6ba22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (enc1): Sequential(\n",
       "    (0): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottom): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec3): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec2): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec1): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Bottom (bottleneck)\n",
    "        self.bottom = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 256, 256, kernel_size=3, padding=1),  # Concatenated channels from enc3\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(128 + 128, 128, kernel_size=3, padding=1),  # Concatenated channels from enc2\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64 + 64, 64, kernel_size=3, padding=1),    # Concatenated channels from enc1\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.enc1(x)   # (64, H, W)\n",
    "        p1 = self.pool1(x1) # (64, H/2, W/2)\n",
    "\n",
    "        x2 = self.enc2(p1)  # (128, H/2, W/2)\n",
    "        p2 = self.pool2(x2) # (128, H/4, W/4)\n",
    "\n",
    "        x3 = self.enc3(p2)  # (256, H/4, W/4)\n",
    "        p3 = self.pool3(x3) # (256, H/8, W/8)\n",
    "\n",
    "        # Bottleneck\n",
    "        x_bottom = self.bottom(p3) # (512, H/8, W/8)\n",
    "\n",
    "        # Decoder\n",
    "        d3 = self.upconv3(x_bottom)        # (256, H/4, W/4)\n",
    "        d3 = torch.cat([d3, x3], dim=1)      # (256 + 256, H/4, W/4)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)              # (128, H/2, W/2)\n",
    "        d2 = torch.cat([d2, x2], dim=1)      # (128 + 128, H/2, W/2)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)              # (64, H, W)\n",
    "        d1 = torch.cat([d1, x1], dim=1)      # (64 + 64, H, W)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        out = self.final_conv(d1)          # (num_classes, H, W)\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "in_channels = 15 + 1 + 18 + 28  # As defined in your FCN\n",
    "num_classes = 3\n",
    "model = UNet(in_channels, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c6195-e61a-4fe4-90f7-4d635ff33609",
   "metadata": {},
   "source": [
    "### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e6451-a130-45e2-ad77-ec213514a3ef",
   "metadata": {},
   "source": [
    "##### DEFINING EARLY STOPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b2233f-304f-4445-a228-af7bfe8aa226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68e818-7b1d-47a8-8593-34fd6f9006e6",
   "metadata": {},
   "source": [
    "##### DEFINING THE LOSS FUNCTION AND OPTIMIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89e926-a27b-446e-baff-9c1f34bab32d",
   "metadata": {},
   "source": [
    "##### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cbb4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MutliModalFCNew import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1acdb76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoilCompositionNet(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(18, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(120, 120))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 3\n",
    "model_veg = VegetationNet()\n",
    "model_elevation = ElevationNet()\n",
    "model_ERA = SoilVariableNet()\n",
    "model_solus = SoilCompositionNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model_veg.to(device)\n",
    "model_elevation.to(device)\n",
    "model_ERA.to(device)\n",
    "model_solus.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "423e86cc-eda3-4ab1-bc06-1c91b15b4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = torch.tensor([0.9579453061638281, 0.019226416768045418, 0.02282827706812652])\n",
    "weights = 1.0 / freq\n",
    "weights = weights / weights.sum()\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)  \n",
    "optimizer = optim.AdamW(list(model.parameters())+list(model_veg.parameters())+list(model_elevation.parameters())+\n",
    "                        list(model_ERA.parameters())+list(model_solus.parameters()), lr=0.01, weight_decay=0.0001)\n",
    "early_stopping = EarlyStopping(patience=15, delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be989f1-8dc4-4fc7-a9bf-c3b931101278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/10  Train Loss: 0.0000  Val Loss: 0.0263\n",
      "0\n",
      "Epoch 2/10  Train Loss: 0.0000  Val Loss: 18.2659\n",
      "0\n",
      "Epoch 3/10  Train Loss: 0.0000  Val Loss: 258.5522\n",
      "0\n",
      "Epoch 4/10  Train Loss: 0.0000  Val Loss: 175.5352\n",
      "0\n",
      "Epoch 5/10  Train Loss: 0.0000  Val Loss: 38.5641\n",
      "0\n",
      "Epoch 6/10  Train Loss: 0.0000  Val Loss: 10.8916\n",
      "0\n",
      "Epoch 7/10  Train Loss: 0.0000  Val Loss: 4.6124\n",
      "0\n",
      "Epoch 8/10  Train Loss: 0.0000  Val Loss: 3.1500\n",
      "0\n",
      "Epoch 9/10  Train Loss: 0.0000  Val Loss: 2.0939\n",
      "0\n",
      "Epoch 10/10  Train Loss: 0.0000  Val Loss: 1.4566\n",
      "The training loop ran for 0.45 minutes for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 15 # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')  # Initialize best validation loss as a large number\n",
    "patience_counter = 0.0 # Counter for epochs with no improvement\n",
    "best_model_veg = \"\"\n",
    "best_model_elevation = \"\"\n",
    "best_model_ERA = \"\"\n",
    "best_model_solus = \"\"\n",
    "best_model = \"\"\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for inputs in train_loader:\n",
    "        print(counter)\n",
    "        counter += 1\n",
    "        labels = inputs[-1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        veg = model_veg(inputs[0].to(device))\n",
    "        elevation = model_elevation(inputs[1].to(device))\n",
    "        era = model_ERA(inputs[3].to(device))\n",
    "        solus = model_solus(inputs[2].to(device))\n",
    "        \n",
    "        outputs = model(torch.cat((veg, elevation, era, solus), dim=1))      # shape: (B, 3, 120, 120)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs[0].size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs in val_loader:\n",
    "            labels = inputs[-1].to(device)\n",
    "            \n",
    "            veg = model_veg(inputs[0].to(device))\n",
    "            elevation = model_elevation(inputs[1].to(device))\n",
    "            era = model_ERA(inputs[3].to(device))\n",
    "            solus = model_solus(inputs[2].to(device))\n",
    "            \n",
    "            outputs = model(torch.cat((veg, elevation, era, solus), dim=1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs[0].size(0)\n",
    "    epoch_val_loss = val_loss / len(val_dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  Train Loss: {epoch_loss:.4f}  Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0  # Reset counter if we have improvement\n",
    "        best_model_veg = model_veg\n",
    "        best_model_elevation = model_elevation\n",
    "        best_model_ERA = model_ERA\n",
    "        best_model_solus = model_solus\n",
    "        best_model = model\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "epochs_run = epoch + 1\n",
    "\n",
    "end = time.time()\n",
    "duration = (end-start)/60\n",
    "print(f\"The training loop ran for {round(duration,2)} minutes for {epochs_run} epochs.\")\n",
    "model_veg = best_model_veg\n",
    "model_elevation = best_model_elevation\n",
    "model_ERA = best_model_ERA\n",
    "bmodel_solus = best_model_solus\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a1f11-ed09-4a2f-bb98-32bcefd46ecb",
   "metadata": {},
   "source": [
    "##### TRAINING LOSS AND VALIDATION LOSS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "287e9885-f0c1-4f46-ab96-94e819b69880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1, epochs_run + 1))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a238a-6687-4180-b33c-830cb8bce86b",
   "metadata": {},
   "source": [
    "### MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e48ddf24-9b19-4eba-b4d5-418760a43350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) for each class.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      list: IoU for each class.\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    # Flatten the tensors for easier computation.\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        union = pred_inds.sum().float() + target_inds.sum().float() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # Alternatively, you might want to append 0.0\n",
    "        else:\n",
    "            ious.append((intersection / union).item())\n",
    "    return ious\n",
    "\n",
    "def compute_dice(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute the Dice coefficient for each class.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      list: Dice coefficient for each class.\n",
    "    \"\"\"\n",
    "    dices = []\n",
    "    # Flatten the tensors.\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        total = pred_inds.sum().float() + target_inds.sum().float()\n",
    "        if total == 0:\n",
    "            dices.append(float('nan'))\n",
    "        else:\n",
    "            dices.append((2 * intersection / total).item())\n",
    "    return dices\n",
    "\n",
    "def compute_confusion_matrix(pred, target, num_classes=3):\n",
    "    \"\"\"\n",
    "    Compute a confusion matrix where the rows correspond to true classes\n",
    "    and the columns correspond to predicted classes.\n",
    "    Args:\n",
    "      pred (torch.Tensor): Predicted tensor of shape (N, H, W)\n",
    "      target (torch.Tensor): Ground truth tensor of shape (N, H, W)\n",
    "      num_classes (int): Number of classes.\n",
    "    Returns:\n",
    "      torch.Tensor: A (num_classes, num_classes) confusion matrix.\n",
    "    \"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    # Use torch.bincount to count occurrences of each (true, pred) pair.\n",
    "    cm = torch.bincount(num_classes * target + pred, minlength=num_classes**2)\n",
    "    cm = cm.reshape(num_classes, num_classes)\n",
    "    return cm\n",
    "\n",
    "def compute_precision_recall_f1(conf_matrix):\n",
    "    \"\"\"\n",
    "    Compute per-class precision, recall, and F1 score from a confusion matrix.\n",
    "    Args:\n",
    "      conf_matrix (torch.Tensor): A (num_classes, num_classes) confusion matrix.\n",
    "    Returns:\n",
    "      tuple: Three lists containing precision, recall, and F1 score for each class.\n",
    "    \"\"\"\n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i].item()\n",
    "        FP = conf_matrix[:, i].sum().item() - TP\n",
    "        FN = conf_matrix[i, :].sum().item() - TP\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else float('nan')\n",
    "        recall    = TP / (TP + FN) if (TP + FN) > 0 else float('nan')\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else float('nan')\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    return precisions, recalls, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e628c978-92f3-41dd-93f0-3e4bfc9c426f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44527/344345815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mveg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_veg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0melevation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_elevation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ERA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AAAI hackathon/MutliModalFCNew.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 453\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    454\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model_veg.eval()\n",
    "model_elevation.eval()\n",
    "model_ERA.eval()\n",
    "model_solus.eval()\n",
    "test_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        labels = inputs[-1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        veg = model_veg(inputs[0].to(device))\n",
    "        elevation = model_elevation(inputs[1].to(device))\n",
    "        era = model_ERA(inputs[3].to(device))\n",
    "        solus = model_solus(inputs[2].to(device))\n",
    "        \n",
    "        outputs = model(torch.cat((veg, elevation, era, solus), dim=1))      # shape: (B, 3, 120, 120)\n",
    "        \n",
    "        test_loss += loss.item() * inputs[0].size(0)\n",
    "        \n",
    "        # Convert raw logits to predicted class labels (shape: (batch_size, H, W))\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Collect predictions and ground truth labels for metric computations\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Calculate average test loss over the dataset\n",
    "test_loss = test_loss / len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Concatenate all the collected predictions and labels along the batch dimension\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "print(all_preds.shape)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "print(all_labels.shape)\n",
    "\n",
    "\n",
    "# Compute metrics using the functions defined earlier\n",
    "ious = compute_iou(all_preds, all_labels, num_classes=3)\n",
    "dices = compute_dice(all_preds, all_labels, num_classes=3)\n",
    "conf_matrix = compute_confusion_matrix(all_preds, all_labels, num_classes=3)\n",
    "precisions, recalls, f1s = compute_precision_recall_f1(conf_matrix)\n",
    "\n",
    "print(\"IoU per class:\", ious)\n",
    "print(\"Dice per class:\", dices)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Precision per class:\", precisions)\n",
    "print(\"Recall per class:\", recalls)\n",
    "print(\"F1 per class:\", f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a8fd5-6e6b-4431-9249-911b466f97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, balanced_accuracy_score\n",
    "\n",
    "# Example tensors (replace these with your actual data)\n",
    "# predictions = torch.randint(0, 3, (366, 120, 120))\n",
    "# true_labels = torch.randint(0, 3, (366, 120, 120))\n",
    "\n",
    "# Flatten the tensors so each pixel is one element\n",
    "pred_flat = all_preds.view(-1)\n",
    "true_flat = all_labels.view(-1)\n",
    "\n",
    "# 1. Pixel Accuracy\n",
    "pixel_accuracy = (pred_flat == true_flat).float().mean().item()\n",
    "print(\"Pixel Accuracy:\", pixel_accuracy)\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2])\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# 3. Precision, Recall, and F1 Score (per class)\n",
    "precision = precision_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "recall = recall_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "f1 = f1_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy(), labels=[0, 1, 2], average=None)\n",
    "print(\"Precision per class:\", precision)\n",
    "print(\"Recall per class:\", recall)\n",
    "print(\"F1 Score per class:\", f1)\n",
    "\n",
    "# 4. Intersection over Union (IoU) for each class\n",
    "def compute_iou(pred, true, num_classes=3):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        # Create masks for the current class\n",
    "        pred_inds = (pred == cls)\n",
    "        true_inds = (true == cls)\n",
    "        # Calculate intersection and union\n",
    "        intersection = (pred_inds & true_inds).sum().float()\n",
    "        union = (pred_inds | true_inds).sum().float()\n",
    "        if union.item() == 0:\n",
    "            ious.append(float('nan'))  # Avoid division by zero if class is not present in the true labels\n",
    "        else:\n",
    "            ious.append((intersection / union).item())\n",
    "    return ious\n",
    "\n",
    "ious = compute_iou(pred_flat, true_flat)\n",
    "print(\"IoU per class:\", ious)\n",
    "\n",
    "# 5. Dice Coefficient for each class\n",
    "def compute_dice(pred, true, num_classes=3, eps=1e-6):\n",
    "    dices = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        true_inds = (true == cls)\n",
    "        intersection = (pred_inds & true_inds).sum().float()\n",
    "        dice = (2 * intersection) / (pred_inds.sum() + true_inds.sum() + eps)\n",
    "        dices.append(dice.item())\n",
    "    return dices\n",
    "\n",
    "dices = compute_dice(pred_flat, true_flat)\n",
    "print(\"Dice Coefficient per class:\", dices)\n",
    "\n",
    "# 6. Cohen's Kappa\n",
    "kappa = cohen_kappa_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy())\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "\n",
    "# 7. Balanced Accuracy\n",
    "balanced_acc = balanced_accuracy_score(true_flat.cpu().numpy(), pred_flat.cpu().numpy())\n",
    "print(\"Balanced Accuracy:\", balanced_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d1956-ab93-414c-9bdd-7683dd0aeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_preds.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f7947-26c6-4340-b29b-adfb6e6c99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(groundtruth_mask, pred_mask):\n",
    "    intersect = torch.sum(pred_mask*groundtruth_mask)\n",
    "    union = torch.sum(pred_mask) + torch.sum(groundtruth_mask) - intersect\n",
    "    xor = torch.sum(groundtruth_mask==pred_mask)\n",
    "    acc = torch.mean(xor/(union + xor - intersect))\n",
    "    return round(acc.item(), 5)\n",
    "    \n",
    "def iou(groundtruth_mask, pred_mask):\n",
    "    intersect = torch.sum(pred_mask*groundtruth_mask)\n",
    "    union = torch.sum(pred_mask) + torch.sum(groundtruth_mask) - intersect\n",
    "    iou = torch.mean(intersect/union)\n",
    "    \n",
    "    return round(iou.item(), 5)\n",
    "\n",
    "def dice_coef(groundtruth_mask, pred_mask):\n",
    "    intersect = torch.sum(pred_mask * groundtruth_mask)\n",
    "    total_sum = torch.sum(pred_mask) + torch.sum(groundtruth_mask)\n",
    "    dice = torch.mean(2 * intersect / total_sum)\n",
    "\n",
    "    return round(dice.item(), 5)  # Convert to Python scalar\n",
    "\n",
    "def precision_score_(groundtruth_mask, pred_mask):\n",
    "    intersect = torch.sum(pred_mask*groundtruth_mask)\n",
    "    total_pixel_pred = torch.sum(pred_mask)\n",
    "    precision = torch.mean(intersect/total_pixel_pred)\n",
    "    return round(precision.item(), 5)\n",
    "\n",
    "def recall_score_(groundtruth_mask, pred_mask):\n",
    "    intersect = torch.sum(pred_mask * groundtruth_mask)  # TP\n",
    "    total_pixel_truth = torch.sum(groundtruth_mask)  # TP + FN\n",
    "    \n",
    "    recall = intersect / total_pixel_truth  # Remove torch.mean()\n",
    "    \n",
    "    return round(recall.item(), 5)  # Convert to scalar and round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ce92e-c453-41b4-8fa3-8c4edcb4f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dice_coef(all_labels, all_preds))\n",
    "print(iou(all_labels, all_preds))\n",
    "print(accuracy(all_labels, all_preds))\n",
    "print(precision_score_(all_labels, all_preds))\n",
    "print(recall_score_(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d165e6-71f6-4bd9-849e-50ceb2732714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c44be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192f0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
